{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec734e-09a9-49b4-90ae-6cc1f5098c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import load\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from os import listdir\n",
    "from os.path import getmtime, exists, isdir, isfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import re\n",
    "\n",
    "#IP2024119   Excel's stuff\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment  \n",
    "from openpyxl.utils import get_column_letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3489078-a836-41f2-868b-441dce6b8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AG20241119: extra notation to identify a bit more easily the different comments.\n",
    "#:\n",
    "##-- If a comment is part of the description of a given step on the code, it starts with ##--\n",
    "#    if the comment is for suggesting/implementing changes in the code, it starts with #\n",
    "\n",
    "\n",
    "\n",
    "### <<<   Global variables and settings section >>>\n",
    "#\n",
    "\n",
    "##-- uses for debug with Jupiter's-cells-system\n",
    "rows_to_show = 1   \n",
    "\n",
    "##-- Syntax to use for missing values:   \n",
    "missing_value = 'n/d'\n",
    "\n",
    "# IP20241125 \n",
    "##-- set adjust for shift from UTC(Slack export timestamp) to ProjectManager's preferred TimeZone \n",
    "timmeshift = 'US/Central'  #IP20241125  chose proper value (! string !)  for TimeZone \n",
    "\n",
    "\n",
    "##-- Do you wish to convert only one certain Slack channel? then type it's name: f.e. - 'general'  \n",
    "#    '' - should be preserved for var  initiation  \n",
    "chosen_channel_name = ''  \n",
    "if len(chosen_channel_name) < 1:\n",
    "    analyze_all_channels = True \n",
    "    print('Channel(s) to analyze: All')\n",
    "else:\n",
    "    analyze_all_channels = False\n",
    "    print('Channel(s) to analyze: ', chosen_channel_name)\n",
    "\n",
    " \n",
    "##-- Generate file with the information of all the Slack channels?:\n",
    "write_all_channels_info = True\n",
    "\n",
    "##-- Generate file with the information of all the Slack users?:\n",
    "write_all_users_info = True\n",
    "\n",
    "\n",
    "##-- Insert path where the LOCAL copy of the GoogleDrive folder is:\n",
    "slackexport_folder_path = \"/home/agds/Documents/RebeccaEverleneTrust/RebeccaEverlene_Slack_export\" #AG\n",
    "#slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short' #IP - to test locally\n",
    "#slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Oct 3 2024 - Nov 9 2024' #IP - to test locally\n",
    "#slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Nov 1 2024 - Nov 30 2024' #IP - to test locally\n",
    "#slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Nov 1 2024 - Nov 30 2024 -shrt' #IP - to test locally\n",
    "\n",
    "##-- Check that slackexport_folder_path exists:  #IP20241123\n",
    "if exists(slackexport_folder_path)==False:\n",
    "    print('Please enter a valid path to the source directory')\n",
    "    continue_analysis = False        #  IP20241124  may be add here abort of entire code? like \"sys.exit()\" ?\n",
    "\n",
    "##-- Insert path where the converted files will be saved:\n",
    "converted_directory = \"/home/agds/Downloads\" #AG\n",
    "#converted_directory = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short' #IP - to test locally\n",
    "#converted_directory = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Oct 3 2024 - Nov 9 2024' #IP - to test locally\n",
    "#converted_directory ='E:\\_RET_slack_export\\RebeccaEverlene Slack export Nov 1 2024 - Nov 30 2024' #IP - to test locally\n",
    "#converted_directory ='E:\\_RET_slack_export\\RebeccaEverlene Slack export Nov 1 2024 - Nov 30 2024 -shrt' #IP - to test locally\n",
    "\n",
    "#\n",
    "converted_directory = f\"{converted_directory}/_JSONs_converted\"\n",
    "\n",
    "##-- Check that     exprt_folder_path  for resulting Excels (JSONs been converted) exists:  #IP20241118\n",
    "if exists(converted_directory)==True:\n",
    "    exprt_folder_path = Path(converted_directory)\n",
    "    if exprt_folder_path.is_dir():\n",
    "        print(f\"The folder 'JSONs_converted' already exists in '{converted_directory.split('JSONs')[0][:-1]}' and it will be overwritten.\") #AG20241120\n",
    "        shutil.rmtree(exprt_folder_path)\n",
    "        \n",
    "Path(f\"{converted_directory}\").mkdir(parents=True, exist_ok=True) #IP20241119\n",
    "\n",
    "\n",
    "#IP20241205\n",
    "##-- Do you wish to show keywords in the cells with separated weekly-report's parts?:\n",
    "key_wrd_text_show = False  # True  # \n",
    "\n",
    "#\n",
    "continue_analysis = True      # IP20241123 moved here,to var's initiating section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd3933-8b35-45a2-a997-01996f347398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanDF():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def replace_empty_space(self, df, column):\n",
    "        \"\"\"Function to replace empty spaces \"\" with the string missing_value for a given column\"\"\"\n",
    "        for i in range(len(df)):\n",
    "            if df.at[i,column] == \"\":\n",
    "                df.at[i,column] = missing_value  \n",
    "                \n",
    "    def replace_NaN(self, df, column):\n",
    "        \"\"\"Function to replace missing values with the string missing_value for a given column \"\"\"\n",
    "        df[column] = df[column].fillna(missing_value)    \n",
    "        \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Function that replaces missing values in all the columns of the df\"\"\"\n",
    "        df = df.replace(pd.NaT, missing_value)\n",
    "        df = df.replace(np.nan, missing_value) \n",
    "        df = df.fillna(missing_value)\n",
    "        return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a6a08-51a3-49b1-a8ce-ad557cfc3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InspectSource():\n",
    "    def __init__(self):\n",
    "        self.slackexport_folder_path = slackexport_folder_path\n",
    "        self.continue_analysis = continue_analysis\n",
    "        self.chosen_channel_name = chosen_channel_name\n",
    "        self.analyze_all_channels = analyze_all_channels\n",
    "        \n",
    "    def check_format_of_json_names(self, list_names):\n",
    "        \"\"\" Iterates over all the json files in a channel's directory, and returns a list with the names of the json files \n",
    "        that have the correct format 'yyyy-mm-dd.json' \"\"\"\n",
    "        list_names_dates = []\n",
    "        for i in range(len(list_names)):\n",
    "            match = re.match(r'(\\d{4})(-)(\\d{2})(-)(\\d{2})(.)(json)',list_names[i])\n",
    "            if match!=None:\n",
    "                list_names_dates.append(list_names[i])\n",
    "        return list_names_dates  \n",
    "    \n",
    "    \n",
    "    def get_channels_names(self):     # AG20241120\n",
    "        \"\"\" Returns a list with the name(s) of the Slack channels to be converted.\n",
    "        If analysing one channel, check that its directory exists, and default to the 0-th element of channels_names:\n",
    "        channels_names = [ chosen_channel_name ] for one channel\n",
    "        channels_names = [channel0, channel1, ...] for all the channels \"\"\"\n",
    "        if self.analyze_all_channels == False:\n",
    "            if exists(f\"{self.slackexport_folder_path}/{self.chosen_channel_name}\")==False:\n",
    "                channels_names = []\n",
    "                print(f\"The source directory for the channel '{self.chosen_channel_name}' was not found in {self.slackexport_folder_path}\")\n",
    "                self.continue_analysis = False\n",
    "            else:\n",
    "                channels_names = [self.chosen_channel_name]\n",
    "        else:\n",
    "            all_in_sourceDir = listdir(self.slackexport_folder_path)\n",
    "            channels_names = [all_in_sourceDir[i] for i in range(len(all_in_sourceDir)) if isdir(f\"{self.slackexport_folder_path}/{all_in_sourceDir[i]}\")==True]\n",
    "            \n",
    "        #AG20241120: Pending to check the format of each channel's name. Having empty spaces in the name can cause problems later. \n",
    "        return channels_names\n",
    "            \n",
    "    \n",
    "    def get_all_channels_json_names(self, channels_names): \n",
    "        \"\"\" \n",
    "        Check the names of json files in all the channels to be converted and stores them in a list:\n",
    "        all_channels_jsonFiles_dates = [ [chosen_channel_name_json0, chosen_channel_name_json1, ...] ] for one exportchannel\n",
    "        all_channels_jsonFiles_dates = [ [channel0_json0, channel0_json1, ...], [channel1_json0, channel1_json1, ...], ... ] for all the channels\n",
    "        \"\"\"\n",
    "        all_channels_jsonFiles_dates = []\n",
    "        for channel in channels_names:\n",
    "            channel_jsonFiles_dates = self.check_format_of_json_names( listdir(f\"{self.slackexport_folder_path}/{channel}\") )\n",
    "            all_channels_jsonFiles_dates.append(channel_jsonFiles_dates)\n",
    "        return all_channels_jsonFiles_dates\n",
    "    \n",
    "    \n",
    "    def check_missing_channels(self, present_channel_names):   #AG20241127\n",
    "        ##-- Get name of channels in channels.json:\n",
    "        expected_channel_names = pd.read_json(f\"{self.slackexport_folder_path}/channels.json\")['name'].values\n",
    "        ##-- Check that all the expected channels are in present channels:\n",
    "        missing_channels = []\n",
    "        for channel in expected_channel_names:\n",
    "            if channel not in present_channel_names:\n",
    "                missing_channels.append(channel)\n",
    "        if len(missing_channels) > 0:\n",
    "            return missing_channels\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def check_expected_files_exists(self):    \n",
    "        if exists(self.slackexport_folder_path)==False:\n",
    "            print('Please enter a valid path to the source directory')\n",
    "            self.continue_analysis = False\n",
    "        else:\n",
    "            #  !!! IP2024118  need to check if exist File \"channels.json\"\n",
    "            ##-- Check that the channels.json files exists:     # AG20241119:\n",
    "            if exists(f\"{self.slackexport_folder_path}/channels.json\")==False:\n",
    "                print('File \"channels.json\" was not found in the source directory')\n",
    "                self.continue_analysis = False\n",
    "             \n",
    "            ##-- Check that the users.json files exists:\n",
    "            if exists(f\"{self.slackexport_folder_path}/users.json\")==False:\n",
    "                print('File \"users.json\" was not found in the source directory')\n",
    "                self.continue_analysis = False\n",
    "    \n",
    "            ##-- Get a list with the name of the channels to be converted:\n",
    "            self.channels_names = self.get_channels_names() #AG20241120: Defined routine in function  \n",
    "    \n",
    "            #IP20241129 it could be (maybe) useful in further with GUI, but not now)\n",
    "            ##-- Check for missing channels in the source directory:       #AG20241127\n",
    "            if self.analyze_all_channels == True:\n",
    "                missing_channels = self.check_missing_channels(self.channels_names)\n",
    "                if missing_channels != None:\n",
    "                    print('The following channels are missing in the source directory:', missing_channels)\n",
    "                    self.continue_analysis = True\n",
    "                    #IP20241129  continue_analysis = False    ##AG: pending to prompt the user if continuing with the analysis? (Relevant for the GUI)\n",
    "\n",
    "            ##-- Get the name of all the json files of the form \"yyyy-mm-dd.json\" in each channel directory:\n",
    "            self.all_channels_jsonFiles_dates = self.get_all_channels_json_names(self.channels_names) # AG20241120: Defined routine in function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b23aca-b1a2-4de4-bf90-d033e527ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlackChannelsAndUsers():\n",
    "    def __init__(self):\n",
    "        self.inspect_source = InspectSource()\n",
    "        self.cleanDF = CleanDF()\n",
    "        self.converted_directory = converted_directory\n",
    "        self.slackexport_folder_path = slackexport_folder_path\n",
    "        self.continue_analysis = continue_analysis\n",
    "        self.chosen_channel_name = chosen_channel_name\n",
    "        self.analyze_all_channels = analyze_all_channels\n",
    "        self.write_all_channels_info = write_all_channels_info\n",
    "        self.write_all_users_info = write_all_users_info\n",
    "\n",
    "    def write_info_to_file(self, write_file_flag, filename, df):\n",
    "        if self.continue_analysis==False:\n",
    "            print(\"Please review the input information\")\n",
    "        else:    \n",
    "            if write_file_flag==True:\n",
    "                slack_export_user_filename = filename        \n",
    "                slack_export_user_folder_path_xlsx = f\"{self.converted_directory}/{slack_export_user_filename}{'.xlsx'}\" #_IP\n",
    "                df.to_excel(slack_export_user_folder_path_xlsx, index=False) #_IP\n",
    "                print(datetime.now().time(), f\"Wrote file {filename}.xlsx\")\n",
    "                \n",
    "    def get_all_channels_info(self):\n",
    "        \"\"\"\n",
    "        This function exports the file channels.json into the dataframe all_channels_df and filters/format relevant features.\n",
    "        The primary features of all_channels_df are: \n",
    "            id, name, created, creator, is_archived, is_general, members, pins, topic, purpose.\n",
    "        The secondary features of 'pins' are:\n",
    "            id, type, created, user, owner.\n",
    "            Generally a list of dictionaries.\n",
    "        The secondary features of 'topic' are:\n",
    "            value, creator, last_set.\n",
    "        \"\"\"\n",
    "        ##-- Export channels.json to dataframe    \n",
    "        self.all_channels_df = pd.read_json(f\"{self.slackexport_folder_path}/channels.json\")\n",
    "    \n",
    "        ##-- Format relevant features on all_channels_df:\n",
    "        all_json_files = []\n",
    "        for i in range(len(self.all_channels_df)):\n",
    "            ##-- Adds df['members']. Writes the list of members into a string separated by commnas:\n",
    "            tmp_list = self.all_channels_df.at[i, 'members']\n",
    "            members_str = \"\".join(f\"{tmp_list[j]}, \" for j in range(len(tmp_list)))\n",
    "            self.all_channels_df.at[i,'members'] = members_str[:-2]\n",
    "            ##-- Adds df['purpose']:\n",
    "            self.all_channels_df.at[i,'purpose'] = self.all_channels_df.at[i,'purpose']['value']\n",
    "            ##-- Adds a list with the channel's json_files with the correct format (yyyy-mm-dd.json):\n",
    "            channel_path = f\"{self.slackexport_folder_path}/{self.all_channels_df.at[i,'name']}\"\n",
    "            \n",
    "            ##-- Check that the channel_path exists:   #IP20241118\n",
    "            if exists(channel_path)==True:\n",
    "                list_names_dates = self.inspect_source.check_format_of_json_names(listdir(channel_path)) #AG20241120: list_names_others not part of the output anymore\n",
    "                all_json_files.append(list_names_dates)\n",
    "            else:\n",
    "                all_json_files.append(missing_value)  \n",
    "        self.all_channels_df['json_files'] = all_json_files\n",
    "        \n",
    "        ##-- Keep the relevant features:\n",
    "        self.all_channels_df = self.all_channels_df[['id', 'name', 'created', 'creator', 'is_archived', 'is_general', 'members', 'purpose', 'json_files']]\n",
    "    \n",
    "        ##-- Handle missing values or empty strings:\n",
    "        for feature in ['members', 'purpose']:\n",
    "            self.cleanDF.replace_empty_space(self.all_channels_df, feature)\n",
    "\n",
    "        ##-- Write all channel's info to .xlsx files, if requested by user:\n",
    "        self.write_info_to_file(self.write_all_channels_info, \"_all_channels\", self.all_channels_df)\n",
    "    \n",
    "    \n",
    "    def get_all_users_info(self):\n",
    "        \"\"\"\n",
    "        This function exports the file users.json into the dataframe all_users_df and filters/format relevant features.\n",
    "        The primary features of all_users_df are: \n",
    "            id, team_id, name, deleted, color, real_name, tz, tz_label, tz_offset, profile, is_admin, is_owner,\n",
    "            is_primary_owner, is_restricted,is_ultra_restricted, is_bot, is_app_user, updated, is_email_confirmed,\n",
    "            who_can_share_contact_card, is_invited_user, is_workflow_bot, is_connector_bot.\n",
    "        Among the secondary features of 'profile', there are:\n",
    "            title, phone, skype, real_name, real_name_normalized, display_name, display_name_normalized, fields, \n",
    "            status_text, status_emoji, status_emoji_display_info, status_expiration, \n",
    "            avatar_hash, image_original, is_custom_image, email, huddle_state, huddle_state_expiration_ts, \n",
    "            first_name, last_name, image_24, image_32, image_48, image_72, image_192, image_512, image_1024, \n",
    "            status_text_canonical, team.\n",
    "        \"\"\"\n",
    "        ##-- Read users.json as a dataframe:\n",
    "        self.all_users_df = pd.read_json(f\"{self.slackexport_folder_path}/users.json\")\n",
    "        \n",
    "        ##-- Keep relevant features on all_users_df:\n",
    "        for i in range(len(self.all_users_df)):\n",
    "            self.all_users_df.at[i, 'display_name'] = self.all_users_df.at[i, 'profile']['display_name']\n",
    "            for feature in ['title', 'real_name', 'status_text', 'status_emoji']:\n",
    "                self.all_users_df.at[i, f\"profile_{feature}\"] = self.all_users_df.at[i, 'profile'][feature]\n",
    "        self.all_users_df = self.all_users_df[['id', 'team_id', 'name', 'deleted', 'display_name', 'is_bot', 'profile_title', 'profile_real_name', \n",
    "                                     'profile_status_text', 'profile_status_emoji']]\n",
    "        \n",
    "        ##-- Handling missing values in all_users_df:\n",
    "        for feature in ['display_name', 'name', 'team_id', 'id', 'profile_title', 'profile_real_name']:#, 'profile_status_text', 'profile_status_emoji']:\n",
    "            self.cleanDF.replace_empty_space(self.all_users_df, feature) \n",
    "            \n",
    "        ##-- Write all users's info to .xlsx files, if requested by user:\n",
    "        self.write_info_to_file(self.write_all_users_info, \"_all_users\", self.all_users_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930f963-e92b-41cd-aa95-921b3c3faf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlackMessages():\n",
    "    def __init__(self):\n",
    "        self.missing_value = missing_value\n",
    "        self.inspect_source = InspectSource()\n",
    "        self.cleanDF = CleanDF()\n",
    "        self.converted_directory = converted_directory\n",
    "        self.slackexport_folder_path = slackexport_folder_path\n",
    "        self.continue_analysis = continue_analysis\n",
    "        self.chosen_channel_name = chosen_channel_name\n",
    "        self.analyze_all_channels = analyze_all_channels\n",
    "        \n",
    "    \n",
    "    def slack_json_to_dataframe(self, slack_json):\n",
    "        \"\"\" Function to extract channel's messages from a JSON file \"\"\"\n",
    "        messages_df = pd.DataFrame(columns=[\"msg_id\", \"ts\", \"user\", \"type\", \"text\", \n",
    "                                            \"reply_count\", \"reply_users_count\", \n",
    "                                            \"ts_latest_reply\", \"ts_thread\", \"parent_user_id\"])\n",
    "        for message in range(len(slack_json)):\n",
    "            #if 'files' in slack_json[message] and slack_json[message]['files']:            #AG:commented out\n",
    "            #    messages_df.at[message, \"msg_id\"] = slack_json[message]['files'][0]['id']  #AG:commented out\n",
    "            if 'client_msg_id' in slack_json[message]:\n",
    "                messages_df.at[message, \"msg_id\"] = slack_json[message]['client_msg_id']\n",
    "            elif 'subtype' in slack_json[message]:                                       #AG:added\n",
    "                messages_df.at[message, \"msg_id\"] = slack_json[message]['subtype']       #AG:added\n",
    "            else:\n",
    "                messages_df.at[message, \"msg_id\"] = missing_value #'n/a'\n",
    "                \n",
    "            #if 'ts' in slack_json[message]:\n",
    "            #    messages_df.at[message, \"ts\"] = slack_json[message]['ts']\n",
    "            #else:\n",
    "            #    messages_df.at[message, \"ts\"] = missing_value  \n",
    "            \n",
    "            #messages_df.at[message, \"user\"] = slack_json[message].get('user', missing_value)  \n",
    "            \n",
    "            #if 'text' in slack_json[message]:\n",
    "            #    messages_df.at[message, \"text\"] = slack_json[message]['text']\n",
    "            #else:\n",
    "            #    messages_df.at[message, \"text\"] = missing_value  \n",
    "    \n",
    "            \n",
    "            # IP20241124 restored (otherwise missed to store timestamps)\n",
    "            if 'type' in slack_json[message]:\n",
    "                messages_df.at[message, \"type\"] = slack_json[message]['type']\n",
    "            else:\n",
    "                messages_df.at[message, \"type\"] = missing_value  \n",
    "    \n",
    "    \n",
    "            # IP20241124 restored (otherwise missed to store timestamps)\n",
    "            if 'reply_count' in slack_json[message]:\n",
    "                #messages_df.at[message, \"reply_count\"] = slack_json[message]['reply_count']   #AG20241127: line could be deleted if using for loop at the end\n",
    "                #messages_df.at[message, \"reply_users_count\"] = slack_json[message]['reply_users_count']  #AG20241127: line could be deleted if using for loop at the end\n",
    "                messages_df.at[message, \"ts_latest_reply\"] = slack_json[message]['latest_reply']\n",
    "            else:\n",
    "                #messages_df.at[message, \"reply_count\"] = missing_value   #AG20241127: line could be deleted if using for loop at the end\n",
    "                #messages_df.at[message, \"reply_users_count\"] = missing_value  #AG20241127: line could be deleted if using for loop at the end\n",
    "                messages_df.at[message, \"ts_latest_reply\"] = missing_value   \n",
    "            \n",
    "            # IP20241124 restored (otherwise missed to store timestamps)\n",
    "            if 'parent_user_id' in slack_json[message]:\n",
    "                messages_df.at[message, \"ts_thread\"] = slack_json[message]['thread_ts']\n",
    "                #messages_df.at[message, \"parent_user_id\"] = slack_json[message]['parent_user_id']  #AG20241127: line could be deleted if using for loop at the end\n",
    "                messages_df.at[message, \"type\"] = \"thread\"    #IP20241124 to distinguish messages and threads\n",
    "            else:\n",
    "                messages_df.at[message, \"ts_thread\"] = missing_value \n",
    "                #messages_df.at[message, \"parent_user_id\"] = missing_value  #AG20241127: line could be deleted if using for loop at the end\n",
    "    \n",
    "            messages_df[\"text\"] = messages_df[\"text\"].astype(str)  #IP20241125  this fixed \"FutureWarning: Setting an item of incompatible dtype is deprecated\" \n",
    "    \n",
    "            #IP20241125 Replace CR and LF in only the 'text' column  \n",
    "            #messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: str(x).replace('\\r\\n\\r\\n', '\\r\\n `rn` ').replace('\\r\\r', '\\r `r` ').replace('\\n\\n', '\\n `n` ') if isinstance(x, str) else x)\n",
    "            #IP20241125 Replace  CR \n",
    "            #messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: str(x).replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "            #IP20241125 Replace  LF        this chosen as optimal variance\n",
    "            #messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: str(x).replace('\\r', ' ') if isinstance(x, str) else x)\n",
    "                \n",
    "            #AG20241122 simplified commented lines shown above to:\n",
    "            features = ['ts', 'user',  'text', 'reply_count', 'reply_users_count',  'parent_user_id']  # IP20241124 :: 'type', 'ts_latest_reply', 'ts_thread' - are removed (otherwise missed to store timestamps) \n",
    "            for feature in features:\n",
    "               messages_df.at[message, feature] = slack_json[message].get(feature, missing_value)    \n",
    "                    \n",
    "        return messages_df\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_channel_messages_df(self, export_path, curr_channel_name, json_list):\n",
    "        \"\"\" Extracts all the messages of a given channel from all its JSON files, and stores them on a data frame \"\"\"\n",
    "        channel_messages_df = pd.DataFrame(columns=[\"msg_id\", \"ts\", \"user\", \"type\", \"text\",\n",
    "                                                    \"reply_count\", \"reply_users_count\",\n",
    "                                                    \"ts_latest_reply\", \"ts_thread\", \"parent_user_id\"])\n",
    "                                                    # ,\"channel_folder\", \"json_name\", \"json_mod_date\"])          #_IP\n",
    "        \n",
    "        ##-- Iterate over JSONs inside the current channel's folder:\n",
    "        for file_day in range(len(json_list)):\n",
    "            filejson_path = f\"{export_path}/{curr_channel_name}/{json_list[file_day]}\" #AG\n",
    "            \n",
    "            with open(filejson_path, encoding='utf-8') as f:\n",
    "                import_file_json = load(f)\n",
    "            import_file_df = self.slack_json_to_dataframe(import_file_json)\n",
    "            import_file_df['json_name'] = json_list[file_day]\n",
    "            import_file_df['json_mod_ts'] = getmtime(filejson_path)  #  un-ZIP of download from Ggl-Drive change ts to the non-sense :: \"1980-01-01 00:00:00\" \n",
    "            \n",
    "            channel_messages_df = pd.concat([channel_messages_df, import_file_df], axis=0, ignore_index=True) \n",
    "        \n",
    "        channel_messages_df['channel_folder'] = curr_channel_name   #IP\n",
    "        return channel_messages_df\n",
    "    \n",
    "    \n",
    "    def get_channel_users_df(self, channel_messages_df, users_df ):\n",
    "        \"\"\"Returns a data frame with the information of the users in current channel\"\"\"\n",
    "        ##-- Initialize channel_users_df as a copy of users_df:\n",
    "        channel_users_df = users_df.copy()\n",
    "        ##-- Find the unique set of users in channel:\n",
    "        channel_users_list = channel_messages_df['user'].unique()\n",
    "        ##-- Collect the indices of the users that are NOT in the channel:\n",
    "        indices_to_drop = [i for i in range(len(users_df)) if users_df.at[i,'id'] not in channel_users_list ]\n",
    "        ##-- Drop the rows on indices_to_drop:\n",
    "        channel_users_df.drop(channel_users_df.index[indices_to_drop], inplace=True)\n",
    "        return channel_users_df\n",
    "\n",
    "    \n",
    "    def add_users_info_to_messages(self, df_messages, df_users):\n",
    "        \"\"\"Uses the user's id in the format U1234567789 from the df_messages to find the \n",
    "        name, display name and if the user is a bot from df_users. \n",
    "        The 'name', 'display_name' and 'is_bot' are then added as columns to df_messages\"\"\"\n",
    "        for index in df_messages.index.values:\n",
    "            i_df = df_users[df_users['id']==df_messages.at[index,'user']]\n",
    "            if i_df['display_name'].shape[0]==0:        ##AG: 'USLACKBOT' is a special case\n",
    "                df_messages.at[index, 'name'] =  df_messages.at[index, 'user']\n",
    "                df_messages.at[index, 'display_name'] =  df_messages.at[index, 'user']\n",
    "                df_messages.at[index, 'is_bot'] =  True\n",
    "                df_messages.at[index, 'deactivated'] =  False    #IP20241121\n",
    "            else:\n",
    "                df_messages.at[index, 'name'] = i_df['name'].values\n",
    "                df_messages.at[index, 'display_name'] = i_df['display_name'].values\n",
    "                df_messages.at[index, 'is_bot'] = i_df['is_bot'].values\n",
    "                df_messages.at[index, 'deactivated'] =  i_df['deleted'].values  #IP20241121\n",
    "            del i_df\n",
    "    \n",
    "    \n",
    "    def ts_to_tz(self, df, original_column_name, new_column_name):\n",
    "        \"\"\"Transforms timestamps in a dataframe's column to dates on the \"US/Central\" timezone\"\"\"\n",
    "        df[original_column_name] = pd.to_numeric(df[original_column_name], errors='coerce')   #_IP\n",
    "        tzs = []\n",
    "        for i in range(len(df)):\n",
    "            i_is_null = pd.Series(df.at[i,original_column_name]).isnull().values[0]    #AG20241120\n",
    "            if i_is_null == True:\n",
    "                #i_date = '0000-00-00 00:00:00'\n",
    "                i_date = self.missing_value\n",
    "            else:\n",
    "                # IP20241119    #IP20241125 introduce a var \"timmeshift\" to adjast timezone from the 1st pfrt of code (easy tocontrol)\n",
    "                i_date = pd.to_datetime(df.at[i,original_column_name], unit='s').tz_localize('UTC').tz_convert(timmeshift) #('US/Central')\n",
    "                i_date = datetime.strftime(i_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "            tzs.append(i_date)\n",
    "        df[[original_column_name]].astype('datetime64[s]')\n",
    "        df[original_column_name] = tzs\n",
    "        df.rename(columns={original_column_name: new_column_name}, inplace=True)\n",
    "        \n",
    "    \n",
    "    def extract_urls(self, df):\n",
    "        \"\"\"Extracts all the url links in df['text'] and stores them as a list in df['URL']\"\"\"\n",
    "        extractor = URLExtract()\n",
    "        #print('len(df) = ',len(df))  #IP20241125\n",
    "        for i in range(len(df)):\n",
    "            urls = []\n",
    "            urls = extractor.find_urls(df.at[i,'text'])\n",
    "            #print('i = ', i , 'len(urls)= ', len(urls), 'urls= ', urls)  #IP20241125\n",
    "            if len(urls)>0:\n",
    "                urls_string = ' ;  '.join(urls)  #IP20241125  to fix  error_\"ValueError: Must have equal len keys and value when setting with an iterable\"\n",
    "                df.at[i,'URL(s)'] = urls_string  #IP20241125 \n",
    "                #print('i = ', i , 'urls= ', urls)  #IP20241125\n",
    "            else:\n",
    "                df.at[i,'URL(s)'] = \"\" # None   IP2024118\n",
    "    \n",
    "    \n",
    "    #IP20241121 :: AG!  it should be \"Add cases where the user_id is not found in users_df.\" >> like preserve original user_ID and added note \"user_not_found\"\n",
    "    #IP20241121 :: AG!  in cases  user's \"display_name\"==\"\", then replace \"user_ID\" with \"user_name\"\n",
    "    #IP20241121  ::  AG! :: should Add cases where the user_id is \"USLACKBOT\" or \"SLACKBOT\".\n",
    "    def user_id_to_name(self, df_messages, df_users):\n",
    "        \"\"\"Replaces the user_id in the format <@U12345678> to the user's display_name in df_messages['text'], which happens\n",
    "        when the user is mentioned in an Slack message through the option @user_name. \n",
    "         If there is no display_name, then 'user_id' is replaced with 'profile_real_name'.\n",
    "         All the bots in df_users have an 'id' and 'profile_real_name' (not necessarily 'name' and 'display_id'). Their profile_real_name are:\n",
    "        Zoom, Google Drive, monday.com, monday.com notifications, GitHub, Google Calendar, Loom, Simple Poll, Figma, \n",
    "        OneDrive and SharePoint, Calendly, Outlook Calendar, Rebecca Everlene Trust Company, Slack Team Emoji, New hire onboarding, \n",
    "        Welcome, Clockify - Clocking in/out, Zapier, Update Your Slack Team Icon, Jira, Google Sheets, Time Off, Trailhead, \n",
    "        Slack Team Emoji Copy, Guru, Guru, Google Calendar, Polly.\n",
    "         Notice that 'USLACKBOT' and 'B043CSZ0FL7' are the only bot messages if df_messages, but they are not in df_users!\n",
    "         In the replacements, the \"<<>>\" are used for clarity on the text, since names can generally have more than one word and many names\n",
    "        can be referenced one after the other, which can lead to confusion when reading.\n",
    "        \"\"\"\n",
    "        for i in range(len(df_messages)):\n",
    "            text = df_messages.at[i,'text']\n",
    "            matches = re.findall(r'<+@[A-Za-z0-9]+>',text)\n",
    "            if len(matches)>0:\n",
    "                for match in matches:\n",
    "                    user = match[2:-1]\n",
    "                    # AG20241122: begin\n",
    "                    if user in df_users['id'].values:\n",
    "                        name = df_users[df_users['id']==user]['display_name'].values[0]\n",
    "                        is_bot = df_users[df_users['id']==user]['is_bot'].values[0]   \n",
    "                        if is_bot==True:\n",
    "                            name = df_users[df_users['id']==user]['profile_real_name'].values[0] + ' (bot)'\n",
    "                        elif name == missing_value:\n",
    "                            name = df_users[df_users['id']==user]['profile_real_name'].values[0]\n",
    "                    else: \n",
    "                        name = f\"{user} (user not found)\"  ## Case for USLACKBOT and B043CSZ0FL7, since they are technically not in df_users!\n",
    "                    # AG20241122: end\n",
    "                    text = re.sub(f\"<@{user}>\", f\"@{name}@\", text)  #AG20241122: Added \"<>\" (see function's documentation) \n",
    "                    \n",
    "                    #IP20241121: AG :: should Add cases where the user_id is not found in users_df.\n",
    "                    #IP20241124:  issue above not solved\n",
    "    \n",
    "                    #IP20241121: AG :: should Add cases where the user_id is \"USLACKBOT\" or \"SLACKBOT\".\n",
    "                    #IP20241124:  issue above not solved (or explane - how solved, if solved). Show cell with examples \n",
    "                df_messages.at[i,'text'] = text\n",
    "    \n",
    "    \n",
    "    #AG20241122: defined routine that was inside user_id_to_name_test to its own function:\n",
    "    def parent_user_id_to_name(self, df_messages, df_users):\n",
    "        # IP20241121   \"parent_user_id\"  substitution\n",
    "        '''Replaces the user_id in the format \"UA5748HE\" to the user's display_name in df_messages['parent_user_id']'''\n",
    "        for i in range(len(df_messages)):\n",
    "            #text1 = df_messages.at[i,'parent_user_id']\n",
    "            #matches = re.findall(r'\\bU[A-Za-z0-9]+\\b',text1)\n",
    "            #if len(matches)>0:\n",
    "            #    for match in matches:\n",
    "            #        user1 = match   \n",
    "                    #print(\"i= \", i, \"user1=\", user1)   #IP20241121:\n",
    "            #        if user1 == \"SLACKBOT\" or user1 == \"USLACKBOT\":\n",
    "            #            continue\n",
    "            #        name1 = df_users[df_users['id']==user1]['display_name'].values[0]\n",
    "            #        text1 = re.sub(f\"{user1}\", f\"{name1}\", text1)\n",
    "                    #IP20241121: should Add cases where the user_id is not found in users_df.\n",
    "            #    df_messages.at[i,'parent_user_id'] = text1\n",
    "    \n",
    "            #AG20241122: Propose simplifying a bit (since 'matches' will always have the one element in df_messages['parent_user_id'])\n",
    "            user = df_messages.at[i,'parent_user_id']\n",
    "            if user!=missing_value:\n",
    "                name = df_users[df_users['id']==user]['display_name'].values\n",
    "                if user in df_users['id'].values:\n",
    "                    is_bot = df_users[df_users['id']==user]['is_bot'].values\n",
    "                    if is_bot==True:\n",
    "                        name = df_users[df_users['id']==user]['profile_real_name'].values + ' (bot)'\n",
    "                    elif name == missing_value:\n",
    "                        name = df_users[df_users['id']==user]['profile_real_name'].values\n",
    "                else:\n",
    "                    name = user+' (user not found)'\n",
    "                df_messages.at[i,'parent_user_id'] = name\n",
    "            \n",
    "    \n",
    "    def channel_id_to_name(self, df_messages, df_users):\n",
    "        \"\"\"Replaces <#channel_id|channel_name> to channel_name in df_messages['text'], which happens\n",
    "        when the channel is mentioned in an Slack message through the option #channel_name\"\"\"\n",
    "        for i in range(len(df_messages)):\n",
    "            text = df_messages.at[i,'text']\n",
    "            matches = re.findall(r'#+[A-Za-z0-9]+\\|',text)\n",
    "            if len(matches)>0:\n",
    "                for match in matches:\n",
    "                    text = re.sub(match, \"\", text)\n",
    "                    text = re.sub(r\"<+\\|\", \"<\", text)\n",
    "                df_messages.at[i,'text'] = text\n",
    "\n",
    "    \n",
    "    def apply_excel_adjustments(self, file_path, curr_channel_name):\n",
    "        \"\"\" Excel file formatting/adjustments with  openpyxl (IP) \"\"\"\n",
    "        wb = load_workbook(file_path)\n",
    "        ws = wb.active\n",
    "        #\n",
    "        ##-- Set the column width\n",
    "        column_widths = {\n",
    "            'A': 12, 'B': 19, 'C': 15,'D': 8, 'E': 35, 'F': 5, 'G': 5, 'H': 17, 'I': 17, 'J': 15, \n",
    "            'K': 19, 'L': 19, 'M': 19, 'N': 13, 'O': 25 , 'P': 7 , 'Q': 6  , 'R': 37      \n",
    "        }\n",
    "        ##-- Apply the column widths\n",
    "        for col, width in column_widths.items():\n",
    "            ws.column_dimensions[col].width = width\n",
    "    \n",
    "    \n",
    "        ##--  Apply font color to all cells in column \n",
    "        font_color = \"0707C5\"  \n",
    "        for cell in ws['E']: \n",
    "            cell.font = Font(color=font_color)\n",
    "        # \n",
    "        font_color = \"c10105\"  \n",
    "        for cell in ws['J']: \n",
    "            cell.font = Font(color=font_color)\n",
    "    \n",
    "    \n",
    "        ##-- Loop through each cell in the column_\"E\" >> 'text'  and replace CR+LF    #IP20241125\n",
    "        #    also, set alignments\n",
    "        for row in ws.iter_rows(min_col=5, max_col=5, min_row=2, max_row=ws.max_row):\n",
    "            for cell in row:\n",
    "                if isinstance(cell.value, str):  # Check if the cell contains text\n",
    "                    # Replace CR (carriage return) and LF (line feed) with a space\n",
    "                    cell.value = cell.value.replace('\\r\\n', ' ').replace('\\r', ' ').replace('\\n\\n', '\\n')\n",
    "                    cell.alignment = Alignment(wrap_text=False, vertical=\"top\", horizontal=\"left\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "        #IP20241120  re-order columns  \n",
    "        #   \n",
    "        # Specify the column to move  \n",
    "        col_to_move_indx = 13    # N-of-clmn==(index)+1\n",
    "        col_to_insert_indx = 4\n",
    "        columns = list(ws.columns) # Get all columns\n",
    "        col_to_move = columns[col_to_move_indx]\n",
    "        col_to_insert = columns[col_to_insert_indx]  \n",
    "        col_data = [cell.value for cell in col_to_move] # Get the data in the column to move\n",
    "        ws.delete_cols(col_to_move_indx+1)  # Remove the column from its current position \n",
    "        ws.insert_cols(col_to_insert_indx)  # Insert the column at the destination position\n",
    "        for row_idx, value in enumerate(col_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "    \n",
    "        col_to_move_indx = 14    # N-of-clmn==(index)+1\n",
    "        col_to_insert_indx = 5\n",
    "        # Get all columns\n",
    "        columns = list(ws.columns)\n",
    "        col_to_move = columns[col_to_move_indx]\n",
    "        col_to_insert = columns[col_to_insert_indx]  \n",
    "        # Get the data in the column to move\n",
    "        col_data = [cell.value for cell in col_to_move]\n",
    "        # Remove the column from its current position \n",
    "        ws.delete_cols(col_to_move_indx+1)\n",
    "        # Insert the column at the destination position\n",
    "        ws.insert_cols(col_to_insert_indx)   \n",
    "        for row_idx, value in enumerate(col_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "    \n",
    "        ##-- re-Set the column width AFTER moving columns  IP20241124 (preserve in code, if further column-moving will be changed)\n",
    "        column_widths = {\n",
    "            'A': 12, 'B': 19, 'C': 15,'D': 19, 'E': 19, 'F': 8, 'G': 35, 'H': 5, 'I': 5, 'J': 17, \n",
    "            'K': 17, 'L': 15, 'M': 19, 'N': 19, 'O': 25 , 'P': 7 , 'Q': 6, 'R': 37    \n",
    "        }\n",
    "        ##-- Apply the column widths\n",
    "        for col, width in column_widths.items():\n",
    "            ws.column_dimensions[col].width = width\n",
    "        \n",
    "        # IP20241124 move \"deactivated\" column\n",
    "        col_to_move_indx = 16    # N-of-clmn==(index)+1\n",
    "        col_to_insert_indx = 6\n",
    "        # Get all columns\n",
    "        columns = list(ws.columns)\n",
    "        col_to_move = columns[col_to_move_indx]\n",
    "        col_to_insert = columns[col_to_insert_indx]  \n",
    "        # Get the data in the column to move\n",
    "        col_data = [cell.value for cell in col_to_move]\n",
    "        # Remove the column from its current position \n",
    "        ws.delete_cols(col_to_move_indx+1)\n",
    "        # Insert the column at the destination position\n",
    "        ws.insert_cols(col_to_insert_indx)   \n",
    "        for row_idx, value in enumerate(col_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "        \n",
    "        \n",
    "        # IP20241124 move \"is_bot\" column\n",
    "        col_to_move_indx = 16    # N-of-clmn==(index)+1\n",
    "        col_to_insert_indx = 7\n",
    "        # Get all columns\n",
    "        columns = list(ws.columns)\n",
    "        col_to_move = columns[col_to_move_indx]\n",
    "        col_to_insert = columns[col_to_insert_indx]  \n",
    "        # Get the data in the column to move\n",
    "        col_data = [cell.value for cell in col_to_move]\n",
    "        # Remove the column from its current position \n",
    "        ws.delete_cols(col_to_move_indx+1)\n",
    "        # Insert the column at the destination position\n",
    "        ws.insert_cols(col_to_insert_indx)   \n",
    "        for row_idx, value in enumerate(col_data, start=1):\n",
    "            ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "    \n",
    "    \n",
    "        ##-- Data align-to-left  IP20241124  (excluding 1st row)\n",
    "        for row in ws.iter_rows(min_col=10, max_col=11, min_row=2, max_row=ws.max_row):\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(horizontal='center')   # 'left'\n",
    "                if isinstance(cell.value, (int, float)):\n",
    "                    cell.font = Font(size=12, bold=True)\n",
    "    \n",
    "     \n",
    "    \n",
    "        \n",
    "        #\n",
    "        #  first row (Row 1) formattings\n",
    "        ##-- Freeze the first row (Row 1)\n",
    "        ws.freeze_panes = 'A2'\n",
    "        ##-- Set font size and bold for the first row\n",
    "        font = Font(size=9, bold=True)\n",
    "        ##-- Set the height of the first row\n",
    "        ws.row_dimensions[1].height = 43 \n",
    "        ##-- Define the RGB color\n",
    "        fill = PatternFill(start_color=\"e7c9fb\", end_color=\"e7c9fb\", fill_type=\"solid\")\n",
    "        ##-- Apply the color, font formatting to the 1st row (Header row)\n",
    "        for cell in ws[1]:\n",
    "            cell.font = font\n",
    "            cell.fill = fill\n",
    "            #cell.alignment = Alignment(wrap_text=True) # Set wrap text for the cells in the first row \n",
    "            cell.alignment = Alignment(wrap_text=True, vertical=\"top\", horizontal=\"left\")\n",
    "     \n",
    "        font_color = \"c10105\"  #IP font_color User_name\n",
    "        for cell in ws['E']: \n",
    "            cell.font = Font(color=font_color)\n",
    "    \n",
    "        #IP20241121  fill_color when  -> \"is_bot\"==True  -> message's \"type\"==\"thread\"\n",
    "        fill_bot = PatternFill(start_color=\"FBBF8F\", end_color=\"FBBF8F\", fill_type=\"solid\")\n",
    "        fill_thread = PatternFill(start_color=\"FBFB99\", end_color=\"FBFB99\", fill_type=\"solid\")\n",
    "        last_row = ws.max_row\n",
    "        for i in range(2, last_row + 1):\n",
    "            if ws[f'g{i}'].value == \"True\" or ws[f'g{i}'].value == True:\n",
    "                for col in ['C', 'D', 'E', 'F', 'G']:\n",
    "                    ws[f'{col}{i}'].fill = fill_bot\n",
    "            if ws[f'H{i}'].value == \"thread\":\n",
    "                for col in ['H', 'I']:\n",
    "                    ws[f'{col}{i}'].fill = fill_thread\n",
    "    \n",
    "    \n",
    "        #IP20241129     \"weekly_report\" separation\n",
    "        ##--    IP20241203  set widths for \"weekly-report\" columns\n",
    "        for col_num in range(19, 33):\n",
    "            col_letter = get_column_letter(col_num)   \n",
    "            ws.column_dimensions[col_letter].width = 25\n",
    "        \n",
    "        ##-- weekly-report keywords setting:\n",
    "        keywrds_wkly_report = [\"Weekly Report:\",\"Project Name:\",\"Working on:\", \"Progress and Roadblocks:\", \"Progress:\", \"Roadblocks:\", \n",
    "                                        \"Plans for the following week:\", \"Meetings:\"]  \n",
    "        ##-- fill for Weekly-Report's titles\n",
    "        fill_wkrep_titles = PatternFill(start_color=\"CDB5B7\", end_color=\"CDB5B7\", fill_type=\"solid\")\n",
    "    \n",
    "        ##-- Get the index/letter of the last used column \n",
    "        lst_col_index = ws.max_column              # IP20241215  !!!           refactor to last filled \"title-of-column-in-1st-row\"\n",
    "        lst_col_lttr = get_column_letter(lst_col_index)\n",
    "        ##-- set columns-titles according to the  keywrds_wkly_report\n",
    "    \n",
    "        wkrep_tech_title = \"weekly-rep-all\"  # column for tech purpose - start-&-end positions of keywords \n",
    "        ws.cell(row=1, column=lst_col_index+1).value = wkrep_tech_title\n",
    "        ws.cell(row=1, column=lst_col_index+1).fill = fill_wkrep_titles\n",
    "        #\n",
    "        for col_idx, value in zip(range(lst_col_index + 2, lst_col_index + len(keywrds_wkly_report) + 2), keywrds_wkly_report):\n",
    "            cell = ws.cell(row=1, column=col_idx)\n",
    "            cell.value = value\n",
    "        \n",
    "            col_letter = get_column_letter(col_idx)  # Convert column number to letter\n",
    "            ws.column_dimensions[col_letter].width = 25\n",
    "            ws[f'{col_letter}1'].fill = fill_wkrep_titles\n",
    "    \n",
    "        ##-- find column for \"text\" of messages\n",
    "        text_to_find = 'text'\n",
    "        for cell in ws[1]:  # Sheet row 1 is accessed using sheet[1]\n",
    "            if text_to_find.lower() in str(cell.value).lower():  # Case-insensitive search\n",
    "                clmn_lttr_text = get_column_letter(cell.column)  # Get the column letter\n",
    "        #print('clmn_letter_text= ',clmn_lttr_text)\n",
    "    \n",
    "        #\n",
    "        ##-- find keywords positions in cells of the column for \"text\" \n",
    "        for i in range(2, last_row + 1):\n",
    "            key_wrds_text = []\n",
    "            cell_value = ws[f'{clmn_lttr_text}{i}'].value\n",
    "            cell_value = str(cell_value).replace(\"*\", \"\")     #IP20241215  replacement all asterrisks in the \"text\"  \n",
    "            if isinstance(cell_value, str):\n",
    "                for keyword in keywrds_wkly_report:\n",
    "                    position = cell_value.lower().find(keyword.lower())   # .lower()\n",
    "                    if position != -1:\n",
    "                        key_wrds_text.append((i, keyword, position, position + len(keyword)))\n",
    "                        key_wrds_text_sorted = sorted(key_wrds_text, key = lambda x: x[2])\n",
    "            if key_wrds_text:\n",
    "                print ( \"i= \", i, \" len(key_wrds_text_sorted)= \", len(key_wrds_text_sorted))\n",
    "                ##-- IP20241215 delete dumb keywords, which are just a part of some complex keyword \n",
    "                if len(key_wrds_text_sorted) > 1 :\n",
    "                    for j in range(len(key_wrds_text_sorted)-1 , 0, -1):   \n",
    "                        print ( \"j= \", j,  key_wrds_text_sorted[j][1])\n",
    "                        if key_wrds_text_sorted[j][2]  >= key_wrds_text_sorted[j-1][2] and key_wrds_text_sorted[j][2]  < key_wrds_text_sorted[j-1][3]:\n",
    "                            print ( \"i= \", i, \"key_wrds to delete= \", key_wrds_text_sorted[j][1] )\n",
    "                            del key_wrds_text_sorted[j]\n",
    "                #\n",
    "                key_wrds_text_sorted_str = '; '.join([f\"'{match[1]}' at {match[2]}-{match[3]}\" for match in key_wrds_text_sorted])\n",
    "                #\n",
    "                ws.cell(row=i, column=lst_col_index+1, value=key_wrds_text_sorted_str)\n",
    "                print(\"i= \", i, \"key_wrds_txt_srtd= \", key_wrds_text_sorted)\n",
    "                #print(\"i= \", i, \"key_wrds_text_sorted_str= \", key_wrds_text_sorted_str)\n",
    "                #\n",
    "                for j in range(0, len(key_wrds_text_sorted)):    #item in key_wrds_text_sorted:\n",
    "                    for cell_1 in ws[1]:   \n",
    "                        title_item = key_wrds_text_sorted[j][1]\n",
    "                        if title_item.lower() in str(cell_1.value).lower():\n",
    "                            item_clmn=cell_1.column\n",
    "                    #for j in range(len(key_wrds_text_sorted) - 1):   \n",
    "                    \n",
    "                    if j + 1  < len(key_wrds_text_sorted):\n",
    "                        next_item = key_wrds_text_sorted[j + 1]\n",
    "                        end_position = next_item[2]\n",
    "                    else: \n",
    "                        next_item = (\"\", \"\", \"\", \"\")\n",
    "                        end_position = len(str(cell_value))\n",
    "                    print(\"i= \", i, \"len(key_wrds_txt_srt)= \", len(key_wrds_text_sorted), \"item_j= \", j, \" item_clmn= \", item_clmn, \"next_item[2]= \", next_item[2], f\" '{key_wrds_text_sorted[j][1]}'; {key_wrds_text_sorted[j][3]}; {end_position}\")\n",
    "                    #\n",
    "                    if key_wrd_text_show != True:    # print or not keyword as a prefix) in tthe cell \n",
    "                        key_wrd_text = ''\n",
    "                    else:\n",
    "                        key_wrd_text = f\"'{key_wrds_text_sorted[j][1]}' \"\n",
    "                    #\n",
    "                    cell_keywrd_value = f\"{key_wrd_text}{cell_value[key_wrds_text_sorted[j][3]: end_position]}\"    #IP20241215\n",
    "                    #IP20241215  Replace CR (carriage return) and LF (line feed) with a space\n",
    "                    cell_keywrd_value = cell_keywrd_value.replace('\\r\\n', ' ').replace('\\r', ' ').replace('\\n\\n', '\\n')\n",
    "                    ws.cell(row=i, column=item_clmn, value = cell_keywrd_value)            #IP20241215\n",
    "                    #ws.cell(row=i, column=item_clmn, value=f\"{key_wrd_text}{cell_value[key_wrds_text_sorted[j][3]: end_position]}\")\n",
    "                     \n",
    "    \n",
    "    \n",
    "        ##-- Delete columns  ::   json_name \tjson_mod_date\tchannel_folder    #IP20241125\n",
    "        #    this columns are tech only, for development and debug, not for PMs\n",
    "        clmns_to_delete = [\"json_name\",\t\"json_mod_date\", \"channel_folder\", wkrep_tech_title]\n",
    "        for clmn_to_delete in clmns_to_delete:\n",
    "            for cell in ws[1]:  \n",
    "                if clmn_to_delete.lower() in str(cell.value).lower():  \n",
    "                    ws.delete_cols(cell.column) \n",
    "                    break\n",
    "    \n",
    "                \t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "    \n",
    "        ##-- re-Set the column width  IP20241205\n",
    "        for col in range(15,36):\n",
    "            clmn_lttr = get_column_letter(col)\n",
    "            ws.column_dimensions[clmn_lttr].width = 25 \n",
    "        \n",
    "        column_widths = {\n",
    "            'msg_id': 12, 'msg_date': 19, 'user': 15,'name': 19, 'display_name': 19, 'deactivated': 7, 'is_bot': 7, \n",
    "            'type':8, 'text':35, 'reply_count': 5, 'reply_users_count': 5, 'latest_reply_date': 17, 'thread_date': 17, 'parent_user_id': 25, 'URL(s)': 37      \n",
    "        }\n",
    "        for col, width in column_widths.items():\n",
    "            for cell in ws[1]:  \n",
    "                if col.lower() == str(cell.value).lower():  \n",
    "                    clmn_lttr = get_column_letter(cell.column)\n",
    "                    ws.column_dimensions[clmn_lttr].width = width \n",
    "                    break\n",
    "            \n",
    "        ##-- Data align-to-top  IP20241215  (excluding 1st row)\n",
    "        for row in ws.iter_rows(min_col=1, max_col=35, min_row=2, max_row=ws.max_row):\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(vertical='top')   \n",
    "    \n",
    "        #\n",
    "        ##-- Rename the sheet\n",
    "        ws_title = curr_channel_name \n",
    "        ws_title = ws_title[:31]\n",
    "        ws.title = ws_title \n",
    "        #\n",
    "        ##-- Save the changes to the Excel file\n",
    "        wb.save(file_path)\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_all_messages_df(self):\n",
    "        if continue_analysis==False:\n",
    "            print(\"Please review the input information\")\n",
    "        else:    \n",
    "            ##-- Iterate over channel's folders:\n",
    "            dfs_list = []\n",
    "            print(datetime.now().time(), 'Starting loop over channels', '\\n')\n",
    "            for i_channel in range(len(channels_names)):\n",
    "        \n",
    "                ##-- Define the name of the current channel and the source path containing its json files:\n",
    "                curr_channel_name = channels_names[i_channel] \n",
    "                parentfolder_path = f\"{self.slackexport_folder_path}/{curr_channel_name}\" \n",
    "                print(curr_channel_name, datetime.now().time(), ' Set-up channel name and path to directory')\n",
    "                \n",
    "                ##-- Collect all the current_channel's messages in channel_messages_df through the function get_channel_messages_df:\n",
    "                json_list = all_channels_jsonFiles_dates[i_channel]\n",
    "                channel_messages_df = self.get_channel_messages_df(self.slackexport_folder_path, curr_channel_name, json_list)  \n",
    "                print(curr_channel_name, datetime.now().time(), ' Collected channel messages from the json files')\n",
    "                #\n",
    "                #IP20241121 move to separate folders-without-messages-JSONs\n",
    "                if len(channel_messages_df)<1:\n",
    "                    print(\"for the folder \",curr_channel_name,\"messages_number= \",len(channel_messages_df),\"there is no channel's folder\", '\\n')\n",
    "                    continue    \n",
    "        \n",
    "                ##-- Collect all the users in the current channel through the function get_channel_users_df:\n",
    "                channel_users_df = self.get_channel_users_df(channel_messages_df, users_df )\n",
    "                print(curr_channel_name, datetime.now().time(), ' Collected users in current channel')\n",
    "                \n",
    "                ##-- Use channel_users_df to fill-in the user's information in channel_messages_df: \n",
    "                self.add_users_info_to_messages(channel_messages_df, channel_users_df)\n",
    "                print(curr_channel_name, datetime.now().time(), ' Included the users information on channel_messages_df')\n",
    "                \n",
    "                ##-- Replace user and team identifiers with their display_names whenever present in a message:\n",
    "                #user_id_to_name(channel_messages_df, users_df) \n",
    "                self.user_id_to_name(channel_messages_df, users_df) \n",
    "                self.channel_id_to_name(channel_messages_df, users_df)\n",
    "                self.parent_user_id_to_name(channel_messages_df, users_df) #AG20241122: routine defined in its own function\n",
    "                print(curr_channel_name, datetime.now().time(), \" User's id replaced by their names in messages\")\n",
    "        \n",
    "                ##-- Extract hyperlinks from messages, if present (extracted as a list; edit if needed):\n",
    "                self.extract_urls(channel_messages_df)\n",
    "                print(curr_channel_name, datetime.now().time(), ' URLs extracted from messages')\n",
    "        \n",
    "                ##-- Change format of the time in seconds to a date in the CST time-zone: (Pending 'ts_latest_reply' and 'ts_thread'!)\n",
    "                #channel_messages_mindate = pd.to_datetime(np.float64(channel_messages_df['ts']), unit='s').min().date()   #AG20241120: Can be deleted\n",
    "                #channel_messages_maxdate = pd.to_datetime(np.float64(channel_messages_df['ts']), unit='s').max().date()   #AG20241120: Can be deleted\n",
    "                self.ts_to_tz(channel_messages_df, 'ts', 'msg_date')\n",
    "                self.ts_to_tz(channel_messages_df, 'json_mod_ts', 'json_mod_date')\n",
    "                self.ts_to_tz(channel_messages_df, 'ts_latest_reply', 'latest_reply_date')\n",
    "                self.ts_to_tz(channel_messages_df, 'ts_thread', 'thread_date')\n",
    "                print('main_analysys ->>',curr_channel_name, \"  \", datetime.now().time(), ' Formated the dates and times in the dataframe')\n",
    "                    \n",
    "                ##-- Reorder the columns in channel_messages_df, if necessary:\n",
    "                #channel_messages_df = channel_messages_df[['channel', 'json_name', 'json_mod_date', 'user', 'name', 'display_name', 'ts', 'msg_id', 'type', 'text']]\n",
    "                #channel_messages_df.index = ['']*len(channel_messages_df)\n",
    "                \n",
    "                ##-- Sort the dataframe by msg_date:\n",
    "                channel_messages_df.sort_values(by='msg_date', inplace=True, ignore_index=True)\n",
    "                \n",
    "                ##-- Write channel_messages_df to a .xlsx file:\n",
    "                channel_messages_mindate = channel_messages_df['msg_date'].min().split(\" \")[0]\n",
    "                channel_messages_maxdate = channel_messages_df['msg_date'].max().split(\" \")[0]\n",
    "                #channel_messages_maxdate = channel_messages_df['msg_date'].max().split(\" \")[0]\n",
    "                channel_messages_filename = f\"{curr_channel_name}_{channel_messages_mindate}_to_{channel_messages_maxdate}\"\n",
    "                channel_messages_folder_path = f\"{converted_directory}/{channel_messages_filename}.xlsx\"\n",
    "                channel_messages_df.to_excel(f\"{channel_messages_folder_path}\", index=False)\n",
    "                self.apply_excel_adjustments(f\"{channel_messages_folder_path}\",curr_channel_name)  #AG: defined this routine in the function apply_excel_adjustments\n",
    "                print(curr_channel_name, datetime.now().time(), ' Wrote curated messages to xlsx files', '\\n')\n",
    "        \n",
    "                dfs_list.append(channel_messages_df)\n",
    "                \n",
    "        print(datetime.now().time(), 'Done')\n",
    "        \n",
    "        return channel_messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f742d1-8332-489d-8709-b89d4eac1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-- Initialize constructor of the class InspectSource:\n",
    "inspect_source = InspectSource()\n",
    "##-- Execute the main function of the class:\n",
    "inspect_source.check_expected_files_exists()\n",
    "##-- Retrieve variables:\n",
    "continue_analysis = inspect_source.continue_analysis\n",
    "channels_names = inspect_source.channels_names\n",
    "all_channels_jsonFiles_dates = inspect_source.all_channels_jsonFiles_dates\n",
    "\n",
    "\n",
    "##-- Initialize constructor of the class SlackChannelAndUsers:\n",
    "scu = SlackChannelsAndUsers()\n",
    "##-- Execute the main functions of the class:\n",
    "scu.get_all_channels_info()\n",
    "scu.get_all_users_info()\n",
    "##-- Retrieve variables:\n",
    "all_channels_df = scu.all_channels_df\n",
    "users_df = scu.all_users_df\n",
    "\n",
    "\n",
    "##-- Initialize constructor of the class SlackMessages:\n",
    "sm = SlackMessages()\n",
    "##-- Execute the main function of the class:\n",
    "channel_messages_df = sm.get_all_messages_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323575c6-896e-40db-b6f5-d735b88a008f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
