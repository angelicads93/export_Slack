{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23326b81-bc35-4147-ae75-377d2e75d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import load\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from os import listdir\n",
    "from os.path import getmtime, exists, isdir, isfile\n",
    "from pathlib import Path\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill\n",
    "from urlextract import URLExtract\n",
    "import re\n",
    "\n",
    "#IP2024119\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45bf9af4-a979-47e4-bf44-b2ec108cb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows demonstrate in executional cells \n",
    "rows_to_show = 1   # uses for debug with Jupiter's-cells-system\n",
    "\n",
    "\n",
    "# !  IP20241118 naming like \"flag_analyze_all\" & \"info_write_all_channels\" is more understandable from glance\n",
    "\n",
    "## Do you wish to convert all the Slack channels?:\n",
    "analyze_all_flag = True   # False            #AG\n",
    "\n",
    "## If not, insert name of Slack channel to convert:\n",
    "exportname = '' # \"general\"\n",
    "\n",
    "## Generate file with the information of all the Slack channels?:\n",
    "write_all_channels_info = True\n",
    "## Generate file with the information of all the Slack users?:\n",
    "write_all_users_info = True\n",
    "\n",
    "\n",
    "#working_directory = \"...\" #AG: not needed, enough with slackexport_folder_path and exporting_directory\n",
    "#working_directory = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2' \n",
    "\n",
    "## Insert path where the LOCAL copy of the GoogleDrive folder is:\n",
    "#slackexport_folder_path = \"/home/agds/Documents/RebeccaEverleneTrust/App/RebeccaEverlene_Slack_export\" #AG\n",
    "slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short' #IP\n",
    "#slackexport_folder_path = f\"{working_directory}\\{exportname}\"\n",
    "\n",
    "\n",
    "# ! IP20241118 if \"exporting_directory\" in fact means \"directory-to-where-to-convert\" \n",
    "#    - it shoild be renamed as \"converted_directory\".  !!  diminish any mess with naming of variables\n",
    "\n",
    "#exporting_directory = \"/home/agds/Downloads\" #AG\n",
    "exporting_directory = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short' #IP\n",
    "\n",
    "exporting_directory = f\"{exporting_directory}/JSONs_converted\"\n",
    "## IP20241119  Check that slackexport_folder_path exists:\n",
    "if exists(exporting_directory)==True:\n",
    "    exprt_folder_path = Path(exporting_directory)\n",
    "    if exprt_folder_path.is_dir():\n",
    "        shutil.rmtree(exprt_folder_path)\n",
    "        \n",
    "Path(f\"{exporting_directory}\").mkdir(parents=True, exist_ok=True) #IP20241119\n",
    "        \n",
    "\n",
    "         \n",
    "#Path(f\"{exporting_directory}/JSONs_converted\").mkdir(parents=True, exist_ok=True) #AG\n",
    "\n",
    "# IP20241118\n",
    "#exporting_directory = f\"{exporting_directory}/RET_converted\"\n",
    "\n",
    "#exporting_directory = f\"{working_directory}\\{'_converted'}\"  \n",
    "\n",
    "# !!!  IP20241118  above is not ok.: code need to check - if the 'RET_converted'  exist.\n",
    "# if exist - delete current 'RET_converted' . \n",
    "# then create new fresh-&-empty 'RET_converted'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "910a5f89-bf2e-46a2-a0da-021fb024e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3d-art-blender-landmarks', '3d-art-team-a-landmarks', '3d-art-team-b-landmarks', 'general', 'JSONs_converted', 'RET_converted', '_converted']\n"
     ]
    }
   ],
   "source": [
    "#AG: Added a routine that checks the existence/format of files/directories of the channel(s) requested by the user.\n",
    "\n",
    "# !!! IP20241118 should check before - is current file a *.json or file extention is different(then skip this file)\n",
    "def check_jsonFile_nameFormat(file_name):\n",
    "    \"\"\" Checks the naming-format of a json file. \n",
    "    Returns True if the name of the file is of the type yyyy-mm-dd.json\"\"\"\n",
    "    list_json_format = file_name.split(\".json\")[0].split(\"-\")   ### COULD BE IMPROVED\n",
    "    try:\n",
    "        if len(list_json_format[0])==4 and len(list_json_format[1])==2 and len(list_json_format[2])==2:\n",
    "            return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def review_format_of_jsonFiles(list_names):\n",
    "    \"\"\" Iterates over all the json files in a channel's directory, and separates the ones with the\n",
    "    correct date-format (list_names_dates) from the rest (list_names_others).\n",
    "    It uses the funtion check_jsonFile_nameFormat \"\"\"\n",
    "    list_names_dates = []\n",
    "    list_names_others = []\n",
    "    for i in range(len(list_names)):\n",
    "        if check_jsonFile_nameFormat(list_names[i])==True:\n",
    "            list_names_dates.append(list_names[i])\n",
    "        else:\n",
    "            list_names_others.append(list_names[i])\n",
    "    return list_names_dates, list_names_others\n",
    "    \n",
    "## Analyze directories:\n",
    "flag_continue = True\n",
    "\n",
    "## Check that slackexport_folder_path exists:\n",
    "if exists(slackexport_folder_path)==False:\n",
    "    print('Please enter a valid path to the source directory')\n",
    "    flag_continue = False\n",
    "else:\n",
    "    ## If analysing one channel, check that its directory exists, and default to the 0-th element of channels_names:\n",
    "    ## channels_names = [ exportname ] for one exportchannel\n",
    "    ## channels_names = [channel0, channel1, ...] for all the channels\n",
    "    if analyze_all_flag == False:\n",
    "        if exists(f\"{slackexport_folder_path}/{exportname}\")==False:\n",
    "            print(f\"The source directory for the channel '{exportname}' was not found in {slackexport_folder_path}\")\n",
    "            flag_continue = False\n",
    "        else:\n",
    "            channels_names = [exportname]\n",
    "    else:\n",
    "        all_in_sourceDir = listdir(slackexport_folder_path)\n",
    "        channels_names = [all_in_sourceDir[i] for i in range(len(all_in_sourceDir)) if isdir(f\"{slackexport_folder_path}/{all_in_sourceDir[i]}\")==True]\n",
    "\n",
    "    \n",
    "    ## Check the names of json files (yyyy-mm-dd.json) and stores them:\n",
    "    ## all_channels_jsonFiles_dates = [ [exportname_json0, exportname_json1, ...] ] for one exportchannel\n",
    "    ## all_channels_jsonFiles_dates = [ [channel0_json0, channel0_json1, ...], [channel1_json0, channel1_json1, ...], ... ] for all the channels\n",
    "    all_channels_jsonFiles_dates = []\n",
    "    #all_channels_jsonFiles_others = []\n",
    "    for channel in channels_names:\n",
    "        channel_jsonFiles_dates, channel_jsonFiles_others = review_format_of_jsonFiles( listdir(f\"{slackexport_folder_path}/{channel}\") )\n",
    "        all_channels_jsonFiles_dates.append(channel_jsonFiles_dates)\n",
    "        #all_channels_jsonFiles_others.append(channel_jsonFiles_others)  \n",
    "        #   IP20241118: \"all_channels_jsonFiles_others.append\"  is senseless, because \"other\" files could have dofferent inner JSON-structure\n",
    "        \n",
    "    ## Check that the users.json files exists:\n",
    "    if exists(f\"{slackexport_folder_path}/users.json\")==False:\n",
    "        print('File \"users.json\" was not found in the source directory')\n",
    "        flag_continue = False\n",
    "\n",
    "    #  !!! IP2024118  need to check if exist File \"channels.json\" \n",
    "\n",
    "\n",
    "#print(flag_continue) \n",
    "print(channels_names)\n",
    "#print(all_channels_jsonFiles_others)\n",
    "#print(all_channels_jsonFiles_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d1aa82b-554d-484c-8824-7a52858b2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of funtions used later in the analysis:\n",
    "\n",
    "# !!! IP20241118  check, why 'n_d'  appears in the \"text\"s cells in messages\n",
    "\n",
    "def replace_empty_space(df, column):\n",
    "    \"\"\"Function to replace empty spaces \"\" with the string 'n/a' for a given column\"\"\"\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i,column] == \"\":\n",
    "            df.at[i,column] = 'n_d'  \n",
    "            \n",
    "def replace_NaN(df, column):\n",
    "    \"\"\"Function to replace missing values with the string 'n/a' for a given column \"\"\"\n",
    "    df[column] = df[column].fillna('n-d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7fbb39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>created</th>\n",
       "      <th>creator</th>\n",
       "      <th>is_archived</th>\n",
       "      <th>is_general</th>\n",
       "      <th>members</th>\n",
       "      <th>pins</th>\n",
       "      <th>topic</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C020HQB61PF</td>\n",
       "      <td>general</td>\n",
       "      <td>1619815937</td>\n",
       "      <td>U02063W7Z1V</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[U02063W7Z1V, U0450DR40FP, U04HGHJ291Q, U04JL6...</td>\n",
       "      <td>[{'id': '1714832626.257419', 'type': 'C', 'cre...</td>\n",
       "      <td>{'value': '', 'creator': '', 'last_set': 0}</td>\n",
       "      <td>{'value': 'This is the one channel that will a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     name     created      creator  is_archived  is_general  \\\n",
       "0  C020HQB61PF  general  1619815937  U02063W7Z1V        False        True   \n",
       "\n",
       "                                             members  \\\n",
       "0  [U02063W7Z1V, U0450DR40FP, U04HGHJ291Q, U04JL6...   \n",
       "\n",
       "                                                pins  \\\n",
       "0  [{'id': '1714832626.257419', 'type': 'C', 'cre...   \n",
       "\n",
       "                                         topic  \\\n",
       "0  {'value': '', 'creator': '', 'last_set': 0}   \n",
       "\n",
       "                                             purpose  \n",
       "0  {'value': 'This is the one channel that will a...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_channels_df = pd.read_json(f\"{slackexport_folder_path}/channels.json\")\n",
    "all_channels_df[0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a38f2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_channels_info(slackexport_folder_path):\n",
    "    \n",
    "    # ! IP20241118   \"The primary features of all_users_df are: \"  - is correct?  may be - \"all_channels_df\" ??\n",
    "    \"\"\"\n",
    "    This function exports the file channels.json into the dataframe all_channels_df and filters/format relevant features.\n",
    "    The primary features of all_users_df are: \n",
    "        id, name, created, creator, is_archived, is_general, members, pins, topic, purpose.\n",
    "    The secondary features of 'pins' are:\n",
    "        id, type, created, user, owner.\n",
    "        Generally a list of dictionaries.\n",
    "    The secondary features of 'topic' are:\n",
    "        value, creator, last_set.\n",
    "    \"\"\"\n",
    "    ### Export channels.json to dataframe    \n",
    "    all_channels_df = pd.read_json(f\"{slackexport_folder_path}/channels.json\")\n",
    "    \n",
    "\n",
    "    # ! IP20241118 code below not take in count - which JSONs _supposed-to-present_ in the export folder\n",
    "    #  code below store only JSONs which physically presented in the time of iterating folder\n",
    "    #  think, all_channels_df should preserv initial list of JSON's, which stored in \"channels.json\" originally\n",
    "    #  to provide manual checking of folder/jsons consistence\n",
    "\n",
    "\n",
    "    ## Format relevant features on all_channels_df:\n",
    "    all_json_files = []\n",
    "    for i in range(len(all_channels_df)):\n",
    "        ## Writes the list of members into a string separated by commnas:\n",
    "        tmp_list = all_channels_df.at[i, 'members']\n",
    "        members_str = \"\".join(f\"{tmp_list[j]}, \" for j in range(len(tmp_list)))\n",
    "        all_channels_df.at[i,'members'] = members_str[:-2]\n",
    "        ## Add the 'purpose' of the channel:\n",
    "        all_channels_df.at[i,'purpose'] = all_channels_df.at[i,'purpose']['value']\n",
    "        ## Adds a list with the channel's json_files with the correct format (yyyy-mm-dd.json):\n",
    "        channel_path = f\"{slackexport_folder_path}/{all_channels_df.at[i,'name']}\"\n",
    "        \n",
    "        #print(\"in the  def'get_all_channels_info' channel_path =>> \"+channel_path )\n",
    "\n",
    "        ## Check that the   channel_path    exists:   #IP20241118\n",
    "        if exists(channel_path)==True:\n",
    "            list_names_dates, list_names_others = review_format_of_jsonFiles(listdir(channel_path))\n",
    "            all_json_files.append(list_names_dates)\n",
    "        else:\n",
    "            all_json_files.append('n/d')\n",
    "\n",
    "        \n",
    "    all_channels_df['json_files'] = all_json_files\n",
    "    \n",
    "    ## Keep the relevant features:\n",
    "    all_channels_df = all_channels_df[['id', 'name', 'created', 'creator', 'is_archived', 'is_general', 'members', 'purpose', 'json_files']]\n",
    "\n",
    "    ## Handle missing values or empty strings:\n",
    "    replace_empty_space(all_channels_df, 'members')\n",
    "    replace_empty_space(all_channels_df, 'purpose')\n",
    "    \n",
    "    return all_channels_df\n",
    "\n",
    "\n",
    "def get_all_users_info(slackexport_folder_path):\n",
    "    \"\"\"\n",
    "    This function exports the file users.json into the dataframe all_users_df and filters/format relevant features.\n",
    "    The primary features of all_users_df are: \n",
    "        id, team_id, name, deleted, color, real_name, tz, tz_label, tz_offset, profile, is_admin, is_owner,\n",
    "        is_primary_owner, is_restricted,is_ultra_restricted, is_bot, is_app_user, updated, is_email_confirmed,\n",
    "        who_can_share_contact_card, is_invited_user, is_workflow_bot, is_connector_bot.\n",
    "    Among the secondary features of 'profile', there are:\n",
    "        title, phone, skype, real_name, real_name_normalized, display_name, display_name_normalized, fields, \n",
    "        status_text, status_emoji, status_emoji_display_info, status_expiration, \n",
    "        avatar_hash, image_original, is_custom_image, email, huddle_state, huddle_state_expiration_ts, \n",
    "        first_name, last_name, image_24, image_32, image_48, image_72, image_192, image_512, image_1024, \n",
    "        status_text_canonical, team.\n",
    "    \"\"\"\n",
    "    ## Read users.json as a dataframe:\n",
    "    all_users_df = pd.read_json(f\"{slackexport_folder_path}/users.json\")\n",
    "    \n",
    "    ## Keep relevant features on all_users_df:\n",
    "    for i in range(len(all_users_df)):\n",
    "        all_users_df.at[i, 'display_name'] = all_users_df.at[i, 'profile']['display_name']\n",
    "        #all_users_df.at[i, 'profile_title'] = all_users_df.at[i, 'profile']['title']  ## Contain a lot of missing values. Display_name seems more representative.\n",
    "    all_users_df = all_users_df[['id', 'team_id', 'name', 'deleted', 'display_name', 'is_bot']]#,'profile_title']]\n",
    "    \n",
    "    ## Handling missing values in all_users_df:\n",
    "    replace_empty_space(all_users_df, 'display_name')\n",
    "    replace_empty_space(all_users_df, 'name')\n",
    "    replace_empty_space(all_users_df, 'team_id')\n",
    "    replace_empty_space(all_users_df, 'id')\n",
    "    \n",
    "    return all_users_df\n",
    "    \n",
    "def slack_json_to_dataframe(slack_json):\n",
    "    \"\"\" Function to extract channel's messages from a JSON file \"\"\"\n",
    "    \n",
    "    messages_df = pd.DataFrame(columns=[\"msg_id\", \"ts\", \"user\", \"type\", \"text\", \n",
    "                                        \"reply_count\", \"reply_users_count\", \n",
    "                                        \"ts_latest_reply\", \"ts_thread\", \"parent_user_id\"])\n",
    "    \n",
    "    # ! IP20241118  \"= None\"  looks not good.  we need explicit sign of data was not provided\n",
    "    #     so, replace in the code ::  \" = None \"  with \" = 'n/d' \"\n",
    "\n",
    "    for message in range(len(slack_json)):\n",
    "        #if 'files' in slack_json[message] and slack_json[message]['files']:            #AG:commented out\n",
    "        #    messages_df.at[message, \"msg_id\"] = slack_json[message]['files'][0]['id']  #AG:commented out\n",
    "        if 'client_msg_id' in slack_json[message]:\n",
    "            messages_df.at[message, \"msg_id\"] = slack_json[message]['client_msg_id']\n",
    "        elif 'subtype' in slack_json[message]:                                       #AG:added\n",
    "            messages_df.at[message, \"msg_id\"] = slack_json[message]['subtype']       #AG:added\n",
    "        else:\n",
    "            messages_df.at[message, \"msg_id\"] = None #'n/a'\n",
    "            \n",
    "        if 'ts' in slack_json[message]:\n",
    "            messages_df.at[message, \"ts\"] = slack_json[message]['ts']\n",
    "        else:\n",
    "            messages_df.at[message, \"ts\"] = None #'n/a'  # 20241110-2\n",
    "            \n",
    "        messages_df.at[message, \"user\"] = slack_json[message].get('user', None) #'n/a')\n",
    "        \n",
    "        if 'type' in slack_json[message]:\n",
    "            messages_df.at[message, \"type\"] = slack_json[message]['type']\n",
    "        else:\n",
    "            messages_df.at[message, \"type\"] = None #'n/a'  # 20241110-2\n",
    "        \n",
    "        if 'text' in slack_json[message]:\n",
    "            messages_df.at[message, \"text\"] = slack_json[message]['text']\n",
    "            #AG:begin\n",
    "            #txt = slack_json[message]['text']\n",
    "            #if txt==\"\" and 'attachments' in slack_json[message]:\n",
    "                ## Covers cases like in 2023/01/11 with the Slackbot messages:\n",
    "            #    messages_df.at[message, \"text\"] = slack_json[message]['attachments'][0]['from_url']\n",
    "                #messages_df.at[message, \"user\"] = slack_json[message]['bot_id']   #overwrittes the empty string, particular to this case.\n",
    "            #else:\n",
    "            #    messages_df.at[message, \"text\"] = txt\n",
    "            #AG:end\n",
    "        else:\n",
    "            messages_df.at[message, \"text\"] = None #'n/a'  # 20241110-2\n",
    "\n",
    "        if 'reply_count' in slack_json[message]:\n",
    "            messages_df.at[message, \"reply_count\"] = slack_json[message]['reply_count']\n",
    "            messages_df.at[message, \"reply_users_count\"] = slack_json[message]['reply_users_count']\n",
    "            messages_df.at[message, \"ts_latest_reply\"] = slack_json[message]['latest_reply']\n",
    "        else:\n",
    "            messages_df.at[message, \"reply_count\"] = None#'n/a'  # 20241110-2\n",
    "            messages_df.at[message, \"reply_users_count\"] = None#'n/a'  # 20241110-2\n",
    "            messages_df.at[message, \"ts_latest_reply\"] = None#'n/a'  # 20241110-2\n",
    "\n",
    "        if 'parent_user_id' in slack_json[message]:\n",
    "            messages_df.at[message, \"ts_thread\"] = slack_json[message]['thread_ts']\n",
    "            messages_df.at[message, \"parent_user_id\"] = slack_json[message]['parent_user_id']\n",
    "        else:\n",
    "            messages_df.at[message, \"ts_thread\"] = None#'n/a'  # 20241110-2\n",
    "            messages_df.at[message, \"parent_user_id\"] = None#'n/a'  # 20241110-2\n",
    "            \n",
    "    return messages_df\n",
    "    \n",
    "\n",
    "def get_channel_messages_df(export_path, curr_channel_name, json_list):\n",
    "    \"\"\" Extracts all the messages of a given channel from its JSON files, and stores them on a data frame \"\"\"\n",
    "    channel_messages_df = pd.DataFrame(columns=[\"msg_id\", \"ts\", \"user\", \"type\", \"text\",\n",
    "                                                \"reply_count\", \"reply_users_count\",\n",
    "                                                \"ts_latest_reply\", \"ts_thread\", \"parent_user_id\"])\n",
    "                                                # ,\"channel_folder\", \"json_name\", \"json_mod_date\"])          #_IP\n",
    "    \n",
    "    # Iterate over JSONs inside the current channel's folder:\n",
    "    for file_day in range(len(json_list)):\n",
    "        #filejson_path = f\"{parentfolder_path}/{channels_json[curr_channel_name]['dayslist'][file_day]}\"\n",
    "        filejson_path = f\"{export_path}/{curr_channel_name}/{json_list[file_day]}\" #AG\n",
    "        \n",
    "        with open(filejson_path, encoding='utf-8') as f:\n",
    "            import_file_json = load(f)\n",
    "        import_file_df = slack_json_to_dataframe(import_file_json)\n",
    "        import_file_df['json_name'] = json_list[file_day]\n",
    "        import_file_df['json_mod_ts'] = getmtime(filejson_path)\n",
    "        \n",
    "        channel_messages_df = pd.concat([channel_messages_df, import_file_df], axis=0, ignore_index=True) \n",
    "    \n",
    "    channel_messages_df['channel_folder'] = curr_channel_name   #IP\n",
    "    return channel_messages_df\n",
    "\n",
    "\n",
    "def get_channel_users_df(channel_messages_df, users_df ):\n",
    "    \"\"\"Returns a data frame with the information of the users in current channel\"\"\"\n",
    "    # Initialize channel_users_df as a copy of users_df:\n",
    "    channel_users_df = users_df.copy()\n",
    "    # Find the unique set of users in channel:\n",
    "    channel_users_list = channel_messages_df['user'].unique()\n",
    "    # Collect the indices of the users that are NOT in the channel:\n",
    "    indices_to_drop = [i for i in range(len(users_df)) if users_df.at[i,'id'] not in channel_users_list ]\n",
    "    # Drop the rows on indices_to_drop:\n",
    "    channel_users_df.drop(channel_users_df.index[indices_to_drop], inplace=True)\n",
    "    return channel_users_df\n",
    "\n",
    "def add_users_info_to_messages(df_messages, df_users):\n",
    "    for index in df_messages.index.values:\n",
    "        i_df = df_users[df_users['id']==df_messages.at[index,'user']]\n",
    "        if i_df['display_name'].shape[0]==0:        ##AG: 'USLACKBOT' is a special case\n",
    "            df_messages.at[index, 'name'] =  df_messages.at[index, 'user']\n",
    "            df_messages.at[index, 'display_name'] =  df_messages.at[index, 'user']\n",
    "            df_messages.at[index, 'is_bot'] =  True\n",
    "        else:\n",
    "            df_messages.at[index, 'name'] = i_df['name'].values\n",
    "            df_messages.at[index, 'display_name'] = i_df['display_name'].values\n",
    "            df_messages.at[index, 'is_bot'] = i_df['is_bot'].values\n",
    "        del i_df\n",
    "    #del channel_users_df\n",
    "\n",
    "\n",
    "\n",
    "# !!!  IP20241118   time convert is not proper. initial time in JSONs is NOT \"EST\" \n",
    "#                       may be - it provided in GMT+0\n",
    "#\n",
    "# !!! encounter problem/bag :: \n",
    "#    \"\" AmbiguousTimeError: Cannot infer dst time from 2023-11-05 01:21:45.201458931, try using the 'ambiguous' argument \"\n",
    "# need to be fixed in the code\n",
    "\n",
    "def ts_to_tz(df, original_column_name, new_column_name):\n",
    "    \"\"\"Transforms timestamps in a dataframe's column to dates on the \"US/Central\" timezone\"\"\"\n",
    "    df[original_column_name] = pd.to_numeric(df[original_column_name], errors='coerce')   #_IP\n",
    "    tzs = []\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i,original_column_name] == None:\n",
    "            i_date = '0000-00-00 00:00:00'\n",
    "        else:\n",
    "            # Convert the time to CST (UTC-6)\n",
    "            #df['datetime_cst'] = df['datetime_gmt'].dt.tz_convert('America/Chicago')\n",
    "\n",
    "            # IP20241119\n",
    "            i_date = pd.to_datetime(df.at[i,original_column_name], unit='s').tz_localize('UTC').tz_convert('US/Central')\n",
    "\n",
    "            #i_date = pd.to_datetime(df.at[i,original_column_name], unit='s').tz_localize('US/Eastern').tz_convert('US/Central')\n",
    "            i_date = datetime.strftime(i_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "        tzs.append(i_date)\n",
    "    df[[original_column_name]].astype('datetime64[s]')\n",
    "    df[original_column_name] = tzs\n",
    "    df.rename(columns={original_column_name: new_column_name}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# IP20241118   should extract not only 1st url from \"text\", but all of url's\n",
    "def extract_urls(df):\n",
    "    \"\"\"\"\"\"\n",
    "    extractor = URLExtract()\n",
    "    for i in range(len(df)):\n",
    "        text = df.at[i,'text']\n",
    "        if 'https' in text:\n",
    "            url = extractor.find_urls(text)\n",
    "            df.at[i,'URL'] = url\n",
    "        else:\n",
    "            df.at[i,'URL'] = '' # None   IP2024118\n",
    "\n",
    "\n",
    "def user_id_to_name(df_messages, df_users):\n",
    "    \"\"\"\"\"\"\n",
    "    for i in range(len(df_messages)):\n",
    "        text = df_messages.at[i,'text']\n",
    "        matches = re.findall(r'<+@[A-Za-z0-9]+>',text)\n",
    "        if len(matches)>0:\n",
    "            for match in matches:\n",
    "                user = match[2:-1]\n",
    "                name = df_users[df_users['id']==user]['display_name'].values[0]\n",
    "                if name!='n/a':\n",
    "                    text = re.sub(f\"<@{user}>\", f\"{name}\", text)\n",
    "            df_messages.at[i,'text'] = text\n",
    "    return df_messages\n",
    "\n",
    "\n",
    "def apply_excel_adjustments(file_path, curr_channel_name):\n",
    "    \"\"\" Excel file formatting/adjustments with  openpyxl (IP) \"\"\"\n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb.active\n",
    "    # Set the column width\n",
    "    column_widths = {\n",
    "        'B': 19, 'C': 15, 'E': 25, 'K': 25, 'L': 19, 'M': 19, 'N': 13, 'O': 13     \n",
    "    }\n",
    "    # Apply the column widths\n",
    "    for col, width in column_widths.items():\n",
    "        ws.column_dimensions[col].width = width\n",
    "    #\n",
    "    # Freeze the first row (Row 1)\n",
    "    ws.freeze_panes = 'A2'\n",
    "    # Set font size and bold for the first row\n",
    "    font = Font(size=9, bold=True)\n",
    "    # Define the RGB color\n",
    "    fill = PatternFill(start_color=\"e7c9fb\", end_color=\"e7c9fb\", fill_type=\"solid\")\n",
    "    # Apply the color to the first row (row 1)\n",
    "    # Apply the font formatting to the first row (Header row)\n",
    "    for cell in ws[1]:\n",
    "        cell.font = font\n",
    "        cell.fill = fill\n",
    "    #\n",
    "    # Rename the sheet\n",
    "    ws_title = curr_channel_name \n",
    "    ws_title = ws_title[:31]\n",
    "    ws.title = ws_title \n",
    "    #\n",
    "    # Save the changes to the Excel file\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ae148b4-3f0b-4f44-9d6b-abe0df76ca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:32:24.391721 Started analysis after sanity checks\n",
      "slackexport_folder_path =>> E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short\n",
      "14:32:24.457562 Obtained channels_df\n",
      "14:32:24.824139 Obtained users_df\n",
      "14:32:24.911788 Wrote channels_df to xlsx file\n",
      "14:32:25.273084 Wrote users_df to xlsx file\n",
      "14:32:25.273084 Starting loop over channels \n",
      "\n",
      "3d-art-blender-landmarks 14:32:25.273084  Set-up channel name and path to directory\n",
      "3d-art-blender-landmarks 14:32:27.757159  Collected channel messages from the json files\n",
      "3d-art-blender-landmarks 14:32:27.790370  Collected users in current channel\n",
      "3d-art-blender-landmarks 14:32:29.290166  Included the users information on channel_messages_df\n",
      "3d-art-blender-landmarks 14:32:29.855785  User's id replaced by their names in messages\n",
      "3d-art-blender-landmarks 14:32:30.513277  URLs extracted from messages\n",
      "main_analysys ->> 3d-art-blender-landmarks 14:32:31.669408  Formated the dates and times in the dataframe\n",
      "3d-art-blender-landmarks 14:32:35.239206  Wrote curated messages to xlsx files \n",
      "\n",
      "3d-art-team-a-landmarks 14:32:35.239206  Set-up channel name and path to directory\n",
      "3d-art-team-a-landmarks 14:32:35.805960  Collected channel messages from the json files\n",
      "3d-art-team-a-landmarks 14:32:35.839301  Collected users in current channel\n",
      "3d-art-team-a-landmarks 14:32:36.176150  Included the users information on channel_messages_df\n",
      "3d-art-team-a-landmarks 14:32:36.239462  User's id replaced by their names in messages\n",
      "3d-art-team-a-landmarks 14:32:36.374156  URLs extracted from messages\n",
      "main_analysys ->> 3d-art-team-a-landmarks 14:32:36.622622  Formated the dates and times in the dataframe\n",
      "3d-art-team-a-landmarks 14:32:37.376621  Wrote curated messages to xlsx files \n",
      "\n",
      "3d-art-team-b-landmarks 14:32:37.376621  Set-up channel name and path to directory\n",
      "3d-art-team-b-landmarks 14:32:37.779050  Collected channel messages from the json files\n",
      "3d-art-team-b-landmarks 14:32:37.805703  Collected users in current channel\n",
      "3d-art-team-b-landmarks 14:32:38.005666  Included the users information on channel_messages_df\n",
      "3d-art-team-b-landmarks 14:32:38.083991  User's id replaced by their names in messages\n",
      "3d-art-team-b-landmarks 14:32:38.222687  URLs extracted from messages\n",
      "main_analysys ->> 3d-art-team-b-landmarks 14:32:38.384892  Formated the dates and times in the dataframe\n",
      "3d-art-team-b-landmarks 14:32:38.922154  Wrote curated messages to xlsx files \n",
      "\n",
      "general 14:32:38.922154  Set-up channel name and path to directory\n",
      "general 14:32:42.772778  Collected channel messages from the json files\n",
      "general 14:32:42.838452  Collected users in current channel\n",
      "general 14:32:45.404910  Included the users information on channel_messages_df\n",
      "general 14:32:46.504806  User's id replaced by their names in messages\n",
      "general 14:32:49.437811  URLs extracted from messages\n",
      "main_analysys ->> general 14:32:51.154410  Formated the dates and times in the dataframe\n",
      "general 14:32:56.303671  Wrote curated messages to xlsx files \n",
      "\n",
      "JSONs_converted 14:32:56.303671  Set-up channel name and path to directory\n",
      "JSONs_converted 14:32:56.303671  Collected channel messages from the json files\n",
      "JSONs_converted 14:32:56.337044  Collected users in current channel\n",
      "JSONs_converted 14:32:56.337044  Included the users information on channel_messages_df\n",
      "JSONs_converted 14:32:56.337044  User's id replaced by their names in messages\n",
      "JSONs_converted 14:32:56.367243  URLs extracted from messages\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'json_mod_ts'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\a500\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'json_mod_ts'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m channel_messages_maxdate \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(np\u001b[38;5;241m.\u001b[39mfloat64(channel_messages_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m]), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mdate()\n\u001b[0;32m     60\u001b[0m ts_to_tz(channel_messages_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg_date\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m \u001b[43mts_to_tz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_messages_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson_mod_ts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson_mod_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_analysys ->>\u001b[39m\u001b[38;5;124m'\u001b[39m,curr_channel_name, datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Formated the dates and times in the dataframe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m## Reorder the columns in channel_messages_df, if necessary:\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m#channel_messages_df = channel_messages_df[['channel', 'json_name', 'json_mod_date', 'user', 'name', 'display_name', 'ts', 'msg_id', 'type', 'text']]\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#channel_messages_df.index = ['']*len(channel_messages_df)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m## Write channel_messages_df to a .xlsx file:\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[70], line 215\u001b[0m, in \u001b[0;36mts_to_tz\u001b[1;34m(df, original_column_name, new_column_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mts_to_tz\u001b[39m(df, original_column_name, new_column_name):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;124;03m\"\"\"Transforms timestamps in a dataframe's column to dates on the \"US/Central\" timezone\"\"\"\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m     df[original_column_name] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)   \u001b[38;5;66;03m#_IP\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     tzs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n",
      "File \u001b[1;32mc:\\Users\\a500\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\a500\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'json_mod_ts'"
     ]
    }
   ],
   "source": [
    "### Main analysis:\n",
    "if flag_continue==False:\n",
    "    print(\"Please review the input information\")\n",
    "else:    \n",
    "    print(datetime.now().time(), 'Started analysis after sanity checks' )\n",
    "    print(\"slackexport_folder_path =>> \"+slackexport_folder_path)\n",
    "    ### Extract the channels and users information into dataframes:\n",
    "    channels_df = get_all_channels_info(slackexport_folder_path)\n",
    "    print(datetime.now().time(), 'Obtained channels_df')\n",
    "    users_df = get_all_users_info(slackexport_folder_path)\n",
    "    print(datetime.now().time(), 'Obtained users_df')\n",
    "    \n",
    "    ### Write all channel's info to .xlsx files, if requested by user:\n",
    "    if write_all_channels_info==True:\n",
    "        slack_export_channel_filename = \"_all_channels\"\n",
    "        slack_export_channel_folder_path_xlsx = f\"{exporting_directory}/{slack_export_channel_filename}{'.xlsx'}\"\n",
    "        channels_df.to_excel(slack_export_channel_folder_path_xlsx, index=False)\n",
    "        print(datetime.now().time(), 'Wrote channels_df to xlsx file')  \n",
    "    \n",
    "    ### Write all users's info to .xlsx files, if requested by user:\n",
    "    if write_all_users_info==True:\n",
    "        slack_export_user_filename = \"_all_users\"        \n",
    "        slack_export_user_folder_path_xlsx = f\"{exporting_directory}/{slack_export_user_filename}{'.xlsx'}\" #_IP\n",
    "        users_df.to_excel(slack_export_user_folder_path_xlsx, index=False) #_IP\n",
    "        print(datetime.now().time(), 'Wrote users_df to xlsx file')\n",
    "\n",
    "    ### Iterate over channel's folders:\n",
    "    print(datetime.now().time(), 'Starting loop over channels', '\\n')\n",
    "    for i_channel in range(len(channels_names)):\n",
    "\n",
    "        ## Define the name of the current channel and the source path containing its json files:\n",
    "        curr_channel_name = channels_names[i_channel] \n",
    "        parentfolder_path = f\"{slackexport_folder_path}/{curr_channel_name}\" \n",
    "        print(curr_channel_name, datetime.now().time(), ' Set-up channel name and path to directory')\n",
    "        \n",
    "        ## Collect all the current_channel's messages in channel_messages_df through the function get_channel_messages_df:\n",
    "        json_list = all_channels_jsonFiles_dates[i_channel]\n",
    "        channel_messages_df = get_channel_messages_df(slackexport_folder_path, curr_channel_name, json_list)  \n",
    "        print(curr_channel_name, datetime.now().time(), ' Collected channel messages from the json files')\n",
    "\n",
    "        ## Collect all the users in the current channel through the function get_channel_users_df:\n",
    "        channel_users_df = get_channel_users_df(channel_messages_df, users_df )\n",
    "        print(curr_channel_name, datetime.now().time(), ' Collected users in current channel')\n",
    "        \n",
    "        ## Use channel_users_df to fill-in the user's information in channel_messages_df:  (define in function onces tested)\n",
    "        add_users_info_to_messages(channel_messages_df, channel_users_df)\n",
    "        print(curr_channel_name, datetime.now().time(), ' Included the users information on channel_messages_df')\n",
    "        \n",
    "        ## Replace user and team identifiers with their display_names whenever present in a message:\n",
    "        user_id_to_name(channel_messages_df, users_df)  #(debugging!)\n",
    "        print(curr_channel_name, datetime.now().time(), \" User's id replaced by their names in messages\")\n",
    "\n",
    "        ## Extract hyperlinks from messages, if present (extracted as a list; edit if needed):\n",
    "        extract_urls(channel_messages_df)\n",
    "        print(curr_channel_name, datetime.now().time(), ' URLs extracted from messages')\n",
    "\n",
    "        ## Change format of the time in seconds to a date in the CST time-zone: (Pending 'ts_latest_reply' and 'ts_thread'!)\n",
    "        channel_messages_mindate = pd.to_datetime(np.float64(channel_messages_df['ts']), unit='s').min().date()\n",
    "        channel_messages_maxdate = pd.to_datetime(np.float64(channel_messages_df['ts']), unit='s').max().date()\n",
    "        ts_to_tz(channel_messages_df, 'ts', 'msg_date')\n",
    "        ts_to_tz(channel_messages_df, 'json_mod_ts', 'json_mod_date')\n",
    "        print('main_analysys ->>',curr_channel_name, datetime.now().time(), ' Formated the dates and times in the dataframe')\n",
    "            \n",
    "        ## Reorder the columns in channel_messages_df, if necessary:\n",
    "        #channel_messages_df = channel_messages_df[['channel', 'json_name', 'json_mod_date', 'user', 'name', 'display_name', 'ts', 'msg_id', 'type', 'text']]\n",
    "        #channel_messages_df.index = ['']*len(channel_messages_df)\n",
    "        \n",
    "        ## Write channel_messages_df to a .xlsx file:\n",
    "        channel_messages_filename = f\"{curr_channel_name}_{channel_messages_mindate}_to_{channel_messages_maxdate}\"\n",
    "        channel_messages_folder_path = f\"{exporting_directory}/{channel_messages_filename}.xlsx\"\n",
    "        channel_messages_df.to_excel(f\"{channel_messages_folder_path}\", index=False)\n",
    "        apply_excel_adjustments(f\"{channel_messages_folder_path}\",curr_channel_name)  #AG: defined this routine in the function apply_excel_adjustments\n",
    "        print(curr_channel_name, datetime.now().time(), ' Wrote curated messages to xlsx files', '\\n')\n",
    "\n",
    "print(datetime.now().time(), 'Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c59bbc-7ee7-4f80-a3e1-f15b122ab821",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f0a31-2b34-4e78-ba24-e8ddc0bc8a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
