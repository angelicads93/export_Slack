{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23326b81-bc35-4147-ae75-377d2e75d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from json import load\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from os import listdir\n",
    "from os.path import getmtime, exists, isdir, isfile\n",
    "from pathlib import Path\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import re\n",
    "\n",
    "#IP2024119   Excel's stuff\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45bf9af4-a979-47e4-bf44-b2ec108cb0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel(s) to analyze:  general\n",
      "The folder 'JSONs_converted' already exists in '/home/agds/Downloads/' and it will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "## AG20241119: Introduced one extra notation to identify a bit more easily the different comments.\n",
    "## If a comment is part of the description of a given step on the code, it starts with ##--\n",
    "## And if the comment is for suggesting/implementing changes in the code, it starts with #\n",
    "\n",
    "rows_to_show = 1   # uses for debug with Jupiter's-cells-system\n",
    "\n",
    "#AG20241120: replaced every explicit reference to 'n/a', None, ... to the global variable 'missing_value'\n",
    "##-- Syntax to use for missing values:   \n",
    "missing_value = 'n/d'\n",
    "\n",
    "# IP20241125 \n",
    "##-- set adjust for shift from UTC(Slack export timestamp) to ProjectManager's preferred TimeZone \n",
    "timmeshift = 'US/Central'  #IP20241125  chose proper value (! string !)  for TimeZone \n",
    "\n",
    "\n",
    "# IP20241125 inserted to the next block  to minimize data input (from 2 to 1 entry) and end-user misfits\n",
    "'''\n",
    "##-- Do you wish to convert all the Slack channels?:\n",
    "analyze_all_channels = False  #True   \n",
    "\n",
    "##-- If not, insert name of Slack channel to convert:\n",
    "'''\n",
    "##-- Do you wish to convert certain only the Slack channel? then type n it's name:\n",
    "#                             f.e. - 'general'   #  '' - should be preserved for var  initiation  \n",
    "chosen_channel_name = 'general'  \n",
    "if len(chosen_channel_name) < 1:\n",
    "    analyze_all_channels = True \n",
    "    print('Channel(s) to analyze: All')\n",
    "else:\n",
    "    analyze_all_channels = False\n",
    "    print('Channel(s) to analyze: ', chosen_channel_name)\n",
    "\n",
    " \n",
    "##-- Generate file with the information of all the Slack channels?:\n",
    "write_all_channels_info = True\n",
    "##-- Generate file with the information of all the Slack users?:\n",
    "write_all_users_info = True\n",
    "\n",
    "\n",
    "##-- Insert path where the LOCAL copy of the GoogleDrive folder is:\n",
    "slackexport_folder_path = \"/home/agds/Documents/RebeccaEverleneTrust/RebeccaEverlene_Slack_export\" #AG\n",
    "#slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short' #IP\n",
    "#slackexport_folder_path = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Oct 3 2024 - Nov 9 2024'\n",
    "\n",
    "##-- Check that slackexport_folder_path exists:  #IP20241123\n",
    "if exists(slackexport_folder_path)==False:\n",
    "    print('Please enter a valid path to the source directory')\n",
    "    continue_analysis = False        #  IP20241124  may be add here abort of entire code? like \"sys.exit()\" ?\n",
    "\n",
    "##-- Insert path where the converted files will be saved:\n",
    "converted_directory = \"/home/agds/Downloads\" #AG\n",
    "#converted_directory = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Apr 30 2021 - Oct 3 2024-2short' #IP\n",
    "#converted_directory = 'E:\\_RET_slack_export\\RebeccaEverlene Slack export Oct 3 2024 - Nov 9 2024'\n",
    "#\n",
    "converted_directory = f\"{converted_directory}/_JSONs_converted\"\n",
    "\n",
    "##-- Check that     exprt_folder_path   exists:  #IP20241118\n",
    "if exists(converted_directory)==True:\n",
    "    exprt_folder_path = Path(converted_directory)\n",
    "    if exprt_folder_path.is_dir():\n",
    "        print(f\"The folder 'JSONs_converted' already exists in '{converted_directory.split('JSONs')[0][:-1]}' and it will be overwritten.\") #AG20241120\n",
    "        shutil.rmtree(exprt_folder_path)\n",
    "        \n",
    "Path(f\"{converted_directory}\").mkdir(parents=True, exist_ok=True) #IP20241119\n",
    "\n",
    "#\n",
    "continue_analysis = True      # IP20241123 moved here,to var's initiating section\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910a5f89-bf2e-46a2-a0da-021fb024e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##--AG: Added a routine that checks the existence/format of files/directories of the channel(s) requested by the user.\n",
    "\n",
    "def check_format_of_json_names(list_names):\n",
    "    \"\"\" Iterates over all the json files in a channel's directory, and returns a list with the names of the json files \n",
    "    that have the correct format 'yyyy-mm-dd.json' \"\"\"\n",
    "    list_names_dates = []\n",
    "    #list_names_others = []\n",
    "    for i in range(len(list_names)):\n",
    "        match = re.match(r'(\\d{4})(-)(\\d{2})(-)(\\d{2})(.)(json)',list_names[i])\n",
    "        if match!=None:\n",
    "            list_names_dates.append(list_names[i])\n",
    "        #else:\n",
    "            #list_names_others.append(list_names[i])    # list_names_others could be deleted : it have not sense (un-unified structures of \"other\".json's).\n",
    "    return list_names_dates            #, list_names_others\n",
    "\n",
    "\n",
    "def get_channels_names(slackexport_folder_path, analyze_all_channels, chosen_channel_name, continue_analysis):     # AG20241120\n",
    "    \"\"\" Returns a list with the name(s) of the Slack channels to be converted.\n",
    "    If analysing one channel, check that its directory exists, and default to the 0-th element of channels_names:\n",
    "    channels_names = [ chosen_channel_name ] for one channel\n",
    "    channels_names = [channel0, channel1, ...] for all the channels \"\"\"\n",
    "    if analyze_all_channels == False:\n",
    "        if exists(f\"{slackexport_folder_path}/{chosen_channel_name}\")==False:\n",
    "            channels_names = []\n",
    "            print(f\"The source directory for the channel '{chosen_channel_name}' was not found in {slackexport_folder_path}\")\n",
    "            continue_analysis = False\n",
    "        else:\n",
    "            channels_names = [chosen_channel_name]\n",
    "    else:\n",
    "        all_in_sourceDir = listdir(slackexport_folder_path)\n",
    "        channels_names = [all_in_sourceDir[i] for i in range(len(all_in_sourceDir)) if isdir(f\"{slackexport_folder_path}/{all_in_sourceDir[i]}\")==True]\n",
    "        \n",
    "    #AG20241120: Pending to check the format of each channel's name. Having empty spaces in the name can cause problems later. \n",
    "    return channels_names\n",
    "        \n",
    "\n",
    "def get_all_channels_json_names(channels_names):     # AG20241120\n",
    "    \"\"\" \n",
    "    Check the names of json files in all the channels to be converted and stores them in a list:\n",
    "    all_channels_jsonFiles_dates = [ [chosen_channel_name_json0, chosen_channel_name_json1, ...] ] for one exportchannel\n",
    "    all_channels_jsonFiles_dates = [ [channel0_json0, channel0_json1, ...], [channel1_json0, channel1_json1, ...], ... ] for all the channels\n",
    "    \"\"\"\n",
    "    all_channels_jsonFiles_dates = []\n",
    "    #all_channels_jsonFiles_others = []\n",
    "    for channel in channels_names:\n",
    "        channel_jsonFiles_dates = check_format_of_json_names( listdir(f\"{slackexport_folder_path}/{channel}\") )\n",
    "        all_channels_jsonFiles_dates.append(channel_jsonFiles_dates)\n",
    "        #all_channels_jsonFiles_others.append(channel_jsonFiles_others)  \n",
    "        #   IP20241118: \"all_channels_jsonFiles_others.append\"  is senseless, because \"other\" files could have dofferent inner JSON-structure\n",
    "        #   AG20241119: Agree. The two commented lines were added to keep track of all the files in every directory, it was used for checks but it can be deleted.    \n",
    "    return all_channels_jsonFiles_dates\n",
    "\n",
    "\n",
    "\n",
    "def check_missing_channels(present_channel_names):   #AG20241127\n",
    "    ##-- Get name of channels in channels.json:\n",
    "    expected_channel_names = pd.read_json(f\"{slackexport_folder_path}/channels.json\")['name'].values\n",
    "    ##-- Check that all the expected channels are in present channels:\n",
    "    missing_channels = []\n",
    "    for channel in expected_channel_names:\n",
    "        if channel not in present_channel_names:\n",
    "            missing_channels.append(channel)\n",
    "    if len(missing_channels) > 0:\n",
    "        return missing_channels\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "##-- Check that slackexport_folder_path exists:\n",
    "if exists(slackexport_folder_path)==False:\n",
    "    print('Please enter a valid path to the source directory')\n",
    "    continue_analysis = False\n",
    "else:\n",
    "    #  !!! IP2024118  need to check if exist File \"channels.json\"\n",
    "    ##-- Check that the channels.json files exists:     # AG20241119:\n",
    "    if exists(f\"{slackexport_folder_path}/channels.json\")==False:\n",
    "        print('File \"channels.json\" was not found in the source directory')\n",
    "        continue_analysis = False\n",
    "     \n",
    "    ##-- Check that the users.json files exists:\n",
    "    if exists(f\"{slackexport_folder_path}/users.json\")==False:\n",
    "        print('File \"users.json\" was not found in the source directory')\n",
    "        continue_analysis = False\n",
    "\n",
    "    ##-- Get a list with the name of the channels to be converted:\n",
    "    channels_names = get_channels_names(slackexport_folder_path, analyze_all_channels, chosen_channel_name, continue_analysis) #AG20241120: Defined routine in function\n",
    "\n",
    "    ##-- Get the name of all the json files of the form \"yyyy-mm-dd.json\" in each channel directory:\n",
    "    all_channels_jsonFiles_dates = get_all_channels_json_names(channels_names) # AG20241120: Defined routine in function\n",
    "    \n",
    "    ##-- Check for missing channels in the source directory:       #AG20241127\n",
    "    if analyze_all_channels == True:\n",
    "        missing_channels = check_missing_channels(channels_names)\n",
    "        if missing_channels != None:\n",
    "            print('The following channels are missing in the source directory:', missing_channels)\n",
    "            continue_analysis = False    ##AG: pending to prompt the user if continuing with the analysis? (Relevant for the GUI)\n",
    "\n",
    "\n",
    "#print(continue_analysis) \n",
    "#print(channels_names)\n",
    "#print(all_channels_jsonFiles_others)\n",
    "#print(all_channels_jsonFiles_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d1aa82b-554d-484c-8824-7a52858b2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition of functions used later in the analysis to replace \"none\" or \"NaN\" entrances:\n",
    "\n",
    "def replace_empty_space(df, column):\n",
    "    \"\"\"Function to replace empty spaces \"\" with the string missing_value for a given column\"\"\"\n",
    "    for i in range(len(df)):\n",
    "        if df.at[i,column] == \"\":\n",
    "            df.at[i,column] = missing_value  \n",
    "            \n",
    "def replace_NaN(df, column):\n",
    "    \"\"\"Function to replace missing values with the string 'n/a' for a given column \"\"\"\n",
    "    df[column] = df[column].fillna(missing_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_channels_df = pd.read_json(f\"{slackexport_folder_path}/channels.json\")\n",
    "#      for visualization, when code run in \"Jupiter-mode\"\n",
    "all_channels_df[0:rows_to_show]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a38f2077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_channels_info(slackexport_folder_path):\n",
    "\n",
    "    \"\"\"\n",
    "    This function exports the file channels.json into the dataframe all_channels_df and filters/format relevant features.\n",
    "    The primary features of all_channels_df are: \n",
    "        id, name, created, creator, is_archived, is_general, members, pins, topic, purpose.\n",
    "    The secondary features of 'pins' are:\n",
    "        id, type, created, user, owner.\n",
    "        Generally a list of dictionaries.\n",
    "    The secondary features of 'topic' are:\n",
    "        value, creator, last_set.\n",
    "    \"\"\"\n",
    "    ##-- Export channels.json to dataframe    \n",
    "    all_channels_df = pd.read_json(f\"{slackexport_folder_path}/channels.json\")\n",
    "\n",
    "    # ! IP20241118 code below not take in count - which JSONs _supposed-to-present_ in the export folder\n",
    "    #  code below store only JSONs which physically presented in the time of iterating folder\n",
    "    #  think, all_channels_df should preserv initial list of JSON's, which stored in \"channels.json\" originally\n",
    "    #  to provide manual checking of folder/jsons consistence\n",
    "\n",
    "    ##-- Format relevant features on all_channels_df:\n",
    "    all_json_files = []\n",
    "    for i in range(len(all_channels_df)):\n",
    "        ##-- Adds df['members']. Writes the list of members into a string separated by commnas:\n",
    "        tmp_list = all_channels_df.at[i, 'members']\n",
    "        members_str = \"\".join(f\"{tmp_list[j]}, \" for j in range(len(tmp_list)))\n",
    "        all_channels_df.at[i,'members'] = members_str[:-2]\n",
    "        ##-- Adds df['purpose']:\n",
    "        all_channels_df.at[i,'purpose'] = all_channels_df.at[i,'purpose']['value']\n",
    "        ##-- Adds a list with the channel's json_files with the correct format (yyyy-mm-dd.json):\n",
    "        channel_path = f\"{slackexport_folder_path}/{all_channels_df.at[i,'name']}\"\n",
    "        \n",
    "        #print(\"in the  def'get_all_channels_info' channel_path =>> \"+channel_path )\n",
    "\n",
    "        ##-- Check that the channel_path exists:   #IP20241118\n",
    "        if exists(channel_path)==True:\n",
    "            list_names_dates = check_format_of_json_names(listdir(channel_path)) #AG20241120: list_names_others not part of the output anymore\n",
    "            all_json_files.append(list_names_dates)\n",
    "        else:\n",
    "            all_json_files.append(missing_value)\n",
    "\n",
    "        \n",
    "    all_channels_df['json_files'] = all_json_files\n",
    "    \n",
    "    ##-- Keep the relevant features:\n",
    "    all_channels_df = all_channels_df[['id', 'name', 'created', 'creator', 'is_archived', 'is_general', 'members', 'purpose', 'json_files']]\n",
    "\n",
    "    ##-- Handle missing values or empty strings:\n",
    "    replace_empty_space(all_channels_df, 'members')\n",
    "    replace_empty_space(all_channels_df, 'purpose')\n",
    "    \n",
    "    return all_channels_df\n",
    "\n",
    "\n",
    "def get_all_users_info(slackexport_folder_path):\n",
    "    \"\"\"\n",
    "    This function exports the file users.json into the dataframe all_users_df and filters/format relevant features.\n",
    "    The primary features of all_users_df are: \n",
    "        id, team_id, name, deleted, color, real_name, tz, tz_label, tz_offset, profile, is_admin, is_owner,\n",
    "        is_primary_owner, is_restricted,is_ultra_restricted, is_bot, is_app_user, updated, is_email_confirmed,\n",
    "        who_can_share_contact_card, is_invited_user, is_workflow_bot, is_connector_bot.\n",
    "    Among the secondary features of 'profile', there are:\n",
    "        title, phone, skype, real_name, real_name_normalized, display_name, display_name_normalized, fields, \n",
    "        status_text, status_emoji, status_emoji_display_info, status_expiration, \n",
    "        avatar_hash, image_original, is_custom_image, email, huddle_state, huddle_state_expiration_ts, \n",
    "        first_name, last_name, image_24, image_32, image_48, image_72, image_192, image_512, image_1024, \n",
    "        status_text_canonical, team.\n",
    "    \"\"\"\n",
    "    ##-- Read users.json as a dataframe:\n",
    "    all_users_df = pd.read_json(f\"{slackexport_folder_path}/users.json\")\n",
    "    \n",
    "    ##-- Keep relevant features on all_users_df:\n",
    "    for i in range(len(all_users_df)):\n",
    "        all_users_df.at[i, 'display_name'] = all_users_df.at[i, 'profile']['display_name']\n",
    "        #IP20241120 :: 'profile_title', 'profile_real_name' should stay, cause support to figure out user, when slack-account had deactivated \n",
    "        #all_users_df.at[i, 'profile_title'] = all_users_df.at[i, 'profile']['title']  ## AG: Contain a lot of missing values. Display_name seems more representative.\n",
    "        #all_users_df.at[i, 'profile_real_name'] = all_users_df.at[i, 'profile']['real_name']  ## AG:  Contain a lot of missing values. Display_name seems more representative.\n",
    "        #IP20241126    to provide additional analytics potential for PMs\n",
    "        #all_users_df.at[i, 'profile_status_text'] = all_users_df.at[i, 'profile']['status_text']\n",
    "        #all_users_df.at[i, 'profile_status_emoji'] = all_users_df.at[i, 'profile']['status_emoji']\n",
    "        # AG20241127: Replaced commented lines above to:\n",
    "        for feature in ['title', 'real_name', 'status_text', 'status_emoji']:\n",
    "            all_users_df.at[i, f\"profile_{feature}\"] = all_users_df.at[i, 'profile'][feature]\n",
    "        #\n",
    "    all_users_df = all_users_df[['id', 'team_id', 'name', 'deleted', 'display_name', 'is_bot', 'profile_title', 'profile_real_name', \n",
    "                                 'profile_status_text', 'profile_status_emoji']]\n",
    "    \n",
    "    ##-- Handling missing values in all_users_df:\n",
    "    #replace_empty_space(all_users_df, 'display_name')\n",
    "    #replace_empty_space(all_users_df, 'name')\n",
    "    #replace_empty_space(all_users_df, 'team_id')\n",
    "    #replace_empty_space(all_users_df, 'id')\n",
    "    #replace_empty_space(all_users_df, 'profile_title')  #IP20241120 \n",
    "    #replace_empty_space(all_users_df, 'profile_real_name')  #IP20241120 \n",
    "    # AG20241127: replaced commented lines above to:\n",
    "    for feature in ['display_name', 'name', 'team_id', 'id', 'profile_title', 'profile_real_name']:#, 'profile_status_text', 'profile_status_emoji']:\n",
    "        replace_empty_space(all_users_df, feature) \n",
    "        \n",
    "    return all_users_df\n",
    "\n",
    "\n",
    "def slack_json_to_dataframe(slack_json):\n",
    "    \"\"\" Function to extract channel's messages from a JSON file \"\"\"\n",
    "    \n",
    "    messages_df = pd.DataFrame(columns=[\"msg_id\", \"ts\", \"user\", \"type\", \"text\", \n",
    "                                        \"reply_count\", \"reply_users_count\", \n",
    "                                        \"ts_latest_reply\", \"ts_thread\", \"parent_user_id\"])\n",
    "    \n",
    "    for message in range(len(slack_json)):\n",
    "        #if 'files' in slack_json[message] and slack_json[message]['files']:            #AG:commented out\n",
    "        #    messages_df.at[message, \"msg_id\"] = slack_json[message]['files'][0]['id']  #AG:commented out\n",
    "        if 'client_msg_id' in slack_json[message]:\n",
    "            messages_df.at[message, \"msg_id\"] = slack_json[message]['client_msg_id']\n",
    "        elif 'subtype' in slack_json[message]:                                       #AG:added\n",
    "            messages_df.at[message, \"msg_id\"] = slack_json[message]['subtype']       #AG:added\n",
    "        else:\n",
    "            messages_df.at[message, \"msg_id\"] = missing_value #'n/a'\n",
    "            \n",
    "        #if 'ts' in slack_json[message]:\n",
    "        #    messages_df.at[message, \"ts\"] = slack_json[message]['ts']\n",
    "        #else:\n",
    "        #    messages_df.at[message, \"ts\"] = missing_value  \n",
    "        \n",
    "        #messages_df.at[message, \"user\"] = slack_json[message].get('user', missing_value)  \n",
    "        \n",
    "        #if 'text' in slack_json[message]:\n",
    "        #    messages_df.at[message, \"text\"] = slack_json[message]['text']\n",
    "        #else:\n",
    "        #    messages_df.at[message, \"text\"] = missing_value  \n",
    "\n",
    "        \n",
    "        # IP20241124 restored (otherwise missed to store timestamps)\n",
    "        if 'type' in slack_json[message]:\n",
    "            messages_df.at[message, \"type\"] = slack_json[message]['type']\n",
    "        else:\n",
    "            messages_df.at[message, \"type\"] = missing_value  \n",
    "\n",
    "\n",
    "        # IP20241124 restored (otherwise missed to store timestamps)\n",
    "        if 'reply_count' in slack_json[message]:\n",
    "            #messages_df.at[message, \"reply_count\"] = slack_json[message]['reply_count']   #AG20241127: line could be deleted if using for loop at the end\n",
    "            #messages_df.at[message, \"reply_users_count\"] = slack_json[message]['reply_users_count']  #AG20241127: line could be deleted if using for loop at the end\n",
    "            messages_df.at[message, \"ts_latest_reply\"] = slack_json[message]['latest_reply']\n",
    "        else:\n",
    "            #messages_df.at[message, \"reply_count\"] = missing_value   #AG20241127: line could be deleted if using for loop at the end\n",
    "            #messages_df.at[message, \"reply_users_count\"] = missing_value  #AG20241127: line could be deleted if using for loop at the end\n",
    "            messages_df.at[message, \"ts_latest_reply\"] = missing_value   \n",
    "        \n",
    "        # IP20241124 restored (otherwise missed to store timestamps)\n",
    "        if 'parent_user_id' in slack_json[message]:\n",
    "            messages_df.at[message, \"ts_thread\"] = slack_json[message]['thread_ts']\n",
    "            #messages_df.at[message, \"parent_user_id\"] = slack_json[message]['parent_user_id']  #AG20241127: line could be deleted if using for loop at the end\n",
    "            messages_df.at[message, \"type\"] = \"thread\"    #IP20241124 to distinguish messages and threads\n",
    "        else:\n",
    "            messages_df.at[message, \"ts_thread\"] = missing_value \n",
    "            #messages_df.at[message, \"parent_user_id\"] = missing_value  #AG20241127: line could be deleted if using for loop at the end\n",
    "\n",
    "        messages_df[\"text\"] = messages_df[\"text\"].astype(str)  #IP20241125  this fixed \"FutureWarning: Setting an item of incompatible dtype is deprecated\" \n",
    "\n",
    "\n",
    "        #IP20241125 Replace CR and LF in only the 'text' column  \n",
    "        #messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: str(x).replace('\\r\\n\\r\\n', '\\r\\n `rn` ').replace('\\r\\r', '\\r `r` ').replace('\\n\\n', '\\n `n` ') if isinstance(x, str) else x)\n",
    "        #IP20241125 Replace  CR \n",
    "        #messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: str(x).replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "        #IP20241125 Replace  LF        this chosen as optimal variance\n",
    "        #messages_df[\"text\"] = messages_df[\"text\"].apply(lambda x: str(x).replace('\\r', ' ') if isinstance(x, str) else x)\n",
    "            \n",
    "\n",
    "        #AG20241122 simplified commented lines shown above to:\n",
    "        features = ['ts', 'user',  'text', 'reply_count', 'reply_users_count',  'parent_user_id']  # IP20241124 :: 'type', 'ts_latest_reply', 'ts_thread' - are removed (otherwise missed to store timestamps) \n",
    "        for feature in features:\n",
    "           messages_df.at[message, feature] = slack_json[message].get(feature, missing_value)    \n",
    "           \n",
    "            \n",
    "    return messages_df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_channel_messages_df(export_path, curr_channel_name, json_list):\n",
    "    \"\"\" Extracts all the messages of a given channel from all its JSON files, and stores them on a data frame \"\"\"\n",
    "    channel_messages_df = pd.DataFrame(columns=[\"msg_id\", \"ts\", \"user\", \"type\", \"text\",\n",
    "                                                \"reply_count\", \"reply_users_count\",\n",
    "                                                \"ts_latest_reply\", \"ts_thread\", \"parent_user_id\"])\n",
    "                                                # ,\"channel_folder\", \"json_name\", \"json_mod_date\"])          #_IP\n",
    "    \n",
    "    ##-- Iterate over JSONs inside the current channel's folder:\n",
    "    for file_day in range(len(json_list)):\n",
    "        filejson_path = f\"{export_path}/{curr_channel_name}/{json_list[file_day]}\" #AG\n",
    "        \n",
    "        with open(filejson_path, encoding='utf-8') as f:\n",
    "            import_file_json = load(f)\n",
    "        import_file_df = slack_json_to_dataframe(import_file_json)\n",
    "        import_file_df['json_name'] = json_list[file_day]\n",
    "        import_file_df['json_mod_ts'] = getmtime(filejson_path)  #  un-ZIP of download from Ggl-Drive change ts to the non-sense :: \"1980-01-01 00:00:00\" \n",
    "        \n",
    "        channel_messages_df = pd.concat([channel_messages_df, import_file_df], axis=0, ignore_index=True) \n",
    "    \n",
    "    channel_messages_df['channel_folder'] = curr_channel_name   #IP\n",
    "    return channel_messages_df\n",
    "\n",
    "\n",
    "def get_channel_users_df(channel_messages_df, users_df ):\n",
    "    \"\"\"Returns a data frame with the information of the users in current channel\"\"\"\n",
    "    ##-- Initialize channel_users_df as a copy of users_df:\n",
    "    channel_users_df = users_df.copy()\n",
    "    ##-- Find the unique set of users in channel:\n",
    "    channel_users_list = channel_messages_df['user'].unique()\n",
    "    ##-- Collect the indices of the users that are NOT in the channel:\n",
    "    indices_to_drop = [i for i in range(len(users_df)) if users_df.at[i,'id'] not in channel_users_list ]\n",
    "    ##-- Drop the rows on indices_to_drop:\n",
    "    channel_users_df.drop(channel_users_df.index[indices_to_drop], inplace=True)\n",
    "    return channel_users_df\n",
    "\n",
    "def add_users_info_to_messages(df_messages, df_users):\n",
    "    \"\"\"Uses the user's id in the format U1234567789 from the df_messages to find the \n",
    "    name, display name and if the user is a bot from df_users. \n",
    "    The 'name', 'display_name' and 'is_bot' are then added as columns to df_messages\"\"\"\n",
    "    for index in df_messages.index.values:\n",
    "        i_df = df_users[df_users['id']==df_messages.at[index,'user']]\n",
    "        if i_df['display_name'].shape[0]==0:        ##AG: 'USLACKBOT' is a special case\n",
    "            df_messages.at[index, 'name'] =  df_messages.at[index, 'user']\n",
    "            df_messages.at[index, 'display_name'] =  df_messages.at[index, 'user']\n",
    "            df_messages.at[index, 'is_bot'] =  True\n",
    "            df_messages.at[index, 'deactivated'] =  False    #IP20241121\n",
    "        else:\n",
    "            df_messages.at[index, 'name'] = i_df['name'].values\n",
    "            df_messages.at[index, 'display_name'] = i_df['display_name'].values\n",
    "            df_messages.at[index, 'is_bot'] = i_df['is_bot'].values\n",
    "            df_messages.at[index, 'deactivated'] =  i_df['deleted'].values  #IP20241121\n",
    "        del i_df\n",
    "\n",
    "\n",
    "def ts_to_tz(df, original_column_name, new_column_name):\n",
    "    \"\"\"Transforms timestamps in a dataframe's column to dates on the \"US/Central\" timezone\"\"\"\n",
    "    df[original_column_name] = pd.to_numeric(df[original_column_name], errors='coerce')   #_IP\n",
    "    tzs = []\n",
    "    for i in range(len(df)):\n",
    "        i_is_null = pd.Series(df.at[i,original_column_name]).isnull().values[0]    #AG20241120\n",
    "        if i_is_null == True:\n",
    "            #i_date = '0000-00-00 00:00:00'\n",
    "            i_date = missing_value\n",
    "        else:\n",
    "            # IP20241119    #IP20241125 introduce a var \"timmeshift\" to adjast timezone from the 1st pfrt of code (easy tocontrol)\n",
    "            i_date = pd.to_datetime(df.at[i,original_column_name], unit='s').tz_localize('UTC').tz_convert(timmeshift) #('US/Central')\n",
    "            i_date = datetime.strftime(i_date,\"%Y-%m-%d %H:%M:%S\")\n",
    "        tzs.append(i_date)\n",
    "    df[[original_column_name]].astype('datetime64[s]')\n",
    "    df[original_column_name] = tzs\n",
    "    df.rename(columns={original_column_name: new_column_name}, inplace=True)\n",
    "    \n",
    "\n",
    "def extract_urls(df):\n",
    "    \"\"\"Extracts all the url links in df['text'] and stores them as a list in df['URL']\"\"\"\n",
    "    extractor = URLExtract()\n",
    "    #print('len(df) = ',len(df))  #IP20241125\n",
    "    for i in range(len(df)):\n",
    "        urls = []\n",
    "        urls = extractor.find_urls(df.at[i,'text'])\n",
    "        #print('i = ', i , 'len(urls)= ', len(urls), 'urls= ', urls)  #IP20241125\n",
    "        if len(urls)>0:\n",
    "            urls_string = ' ;  '.join(urls)  #IP20241125  to fix  error_\"ValueError: Must have equal len keys and value when setting with an iterable\"\n",
    "            df.at[i,'URL(s)'] = urls_string  #IP20241125 \n",
    "            #print('i = ', i , 'urls= ', urls)  #IP20241125\n",
    "        else:\n",
    "            df.at[i,'URL(s)'] = \"\" # None   IP2024118\n",
    "\n",
    "\n",
    "#IP20241121 :: AG!  it should be \"Add cases where the user_id is not found in users_df.\" >> like preserve original user_ID and added note \"user_not_found\"\n",
    "#IP20241121 :: AG!  in cases  user's \"display_name\"==\"\", then replace \"user_ID\" with \"user_name\"\n",
    "#IP20241121  ::  AG! :: should Add cases where the user_id is \"USLACKBOT\" or \"SLACKBOT\".\n",
    "def user_id_to_name(df_messages, df_users):\n",
    "    \"\"\"Replaces the user_id in the format <@U12345678> to the user's display_name in df_messages['text'], which happens\n",
    "    when the user is mentioned in an Slack message through the option @user_name. \n",
    "     If there is no display_name, then 'user_id' is replaced with 'profile_real_name'.\n",
    "     All the bots in df_users have an 'id' and 'profile_real_name' (not necessarily 'name' and 'display_id'). Their profile_real_name are:\n",
    "    Zoom, Google Drive, monday.com, monday.com notifications, GitHub, Google Calendar, Loom, Simple Poll, Figma, \n",
    "    OneDrive and SharePoint, Calendly, Outlook Calendar, Rebecca Everlene Trust Company, Slack Team Emoji, New hire onboarding, \n",
    "    Welcome, Clockify - Clocking in/out, Zapier, Update Your Slack Team Icon, Jira, Google Sheets, Time Off, Trailhead, \n",
    "    Slack Team Emoji Copy, Guru, Guru, Google Calendar, Polly.\n",
    "     Notice that 'USLACKBOT' and 'B043CSZ0FL7' are the only bot messages if df_messages, but they are not in df_users!\n",
    "     In the replacements, the \"<<>>\" are used for clarity on the text, since names can generally have more than one word and many names\n",
    "    can be referenced one after the other, which can lead to confusion when reading.\n",
    "    \"\"\"\n",
    "    for i in range(len(df_messages)):\n",
    "        text = df_messages.at[i,'text']\n",
    "        matches = re.findall(r'<+@[A-Za-z0-9]+>',text)\n",
    "        if len(matches)>0:\n",
    "            for match in matches:\n",
    "                user = match[2:-1]\n",
    "                # AG20241122: begin\n",
    "                if user in df_users['id'].values or user in ['USLACKBOT','B043CSZ0FL7']:\n",
    "                    name = df_users[df_users['id']==user]['display_name'].values[0]\n",
    "                    is_bot = df_users[df_users['id']==user]['is_bot'].values[0]   \n",
    "                    if is_bot==True:\n",
    "                        name = df_users[df_users['id']==user]['profile_real_name'].values[0] + ' (bot)'\n",
    "                    elif name == missing_value:\n",
    "                        name = df_users[df_users['id']==user]['profile_real_name'].values[0]\n",
    "                else: \n",
    "                    name = f\"{user} (user not found)\"  ## Case for USLACKBOT and B043CSZ0FL7, since they are technically not in df_users!\n",
    "                # AG20241122: end\n",
    "                text = re.sub(f\"<@{user}>\", f\"@{name}@\", text)  #AG20241122: Added \"<>\" (see function's documentation) \n",
    "                \n",
    "                #IP20241121: AG :: should Add cases where the user_id is not found in users_df.\n",
    "                #IP20241124:  issue above not solved\n",
    "\n",
    "                #IP20241121: AG :: should Add cases where the user_id is \"USLACKBOT\" or \"SLACKBOT\".\n",
    "                #IP20241124:  issue above not solved (or explane - how solved, if solved). Show cell with examples \n",
    "            df_messages.at[i,'text'] = text\n",
    "\n",
    "\n",
    "#AG20241122: defined routine that was inside user_id_to_name_test to its own function:\n",
    "def parent_user_id_to_name(df_messages, df_users):\n",
    "    # IP20241121   \"parent_user_id\"  substitution\n",
    "    '''Replaces the user_id in the format \"UA5748HE\" to the user's display_name in df_messages['parent_user_id']'''\n",
    "    for i in range(len(df_messages)):\n",
    "        #text1 = df_messages.at[i,'parent_user_id']\n",
    "        #matches = re.findall(r'\\bU[A-Za-z0-9]+\\b',text1)\n",
    "        #if len(matches)>0:\n",
    "        #    for match in matches:\n",
    "        #        user1 = match   \n",
    "                #print(\"i= \", i, \"user1=\", user1)   #IP20241121:\n",
    "        #        if user1 == \"SLACKBOT\" or user1 == \"USLACKBOT\":\n",
    "        #            continue\n",
    "        #        name1 = df_users[df_users['id']==user1]['display_name'].values[0]\n",
    "        #        text1 = re.sub(f\"{user1}\", f\"{name1}\", text1)\n",
    "                #IP20241121: should Add cases where the user_id is not found in users_df.\n",
    "        #    df_messages.at[i,'parent_user_id'] = text1\n",
    "\n",
    "        #AG20241122: Propose simplifying a bit (since 'matches' will always have the one element in df_messages['parent_user_id'])\n",
    "        user = df_messages.at[i,'parent_user_id']\n",
    "        if user!=missing_value:\n",
    "            name = df_users[df_users['id']==user]['display_name'].values\n",
    "            if user in df_users['id'].values:\n",
    "                is_bot = df_users[df_users['id']==user]['is_bot'].values\n",
    "                if is_bot==True:\n",
    "                    name = df_users[df_users['id']==user]['profile_real_name'].values + ' (bot)'\n",
    "                elif name == missing_value:\n",
    "                    name = df_users[df_users['id']==user]['profile_real_name'].values\n",
    "            else:\n",
    "                name = user+' (user not found)'\n",
    "            df_messages.at[i,'parent_user_id'] = name\n",
    "        \n",
    "\n",
    "def channel_id_to_name(df_messages, df_users):\n",
    "    \"\"\"Replaces <#channel_id|channel_name> to channel_name in df_messages['text'], which happens\n",
    "    when the channel is mentioned in an Slack message through the option #channel_name\"\"\"\n",
    "    for i in range(len(df_messages)):\n",
    "        text = df_messages.at[i,'text']\n",
    "        matches = re.findall(r'#+[A-Za-z0-9]+\\|',text)\n",
    "        if len(matches)>0:\n",
    "            for match in matches:\n",
    "                text = re.sub(match, \"\", text)\n",
    "                text = re.sub(r\"<+\\|\", \"<\", text)\n",
    "            df_messages.at[i,'text'] = text\n",
    "\n",
    "\n",
    "\n",
    "def apply_excel_adjustments(file_path, curr_channel_name):\n",
    "    \"\"\" Excel file formatting/adjustments with  openpyxl (IP) \"\"\"\n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb.active\n",
    "    #\n",
    "    ##-- Set the column width\n",
    "    column_widths = {\n",
    "        'A': 12, 'B': 19, 'C': 15,'D': 8, 'E': 35, 'F': 5, 'G': 5, 'H': 17, 'I': 17, 'J': 15, \n",
    "        'K': 19, 'L': 19, 'M': 19, 'N': 13, 'O': 25 , 'P': 7 , 'Q': 6  , 'R': 37      \n",
    "    }\n",
    "    ##-- Apply the column widths\n",
    "    for col, width in column_widths.items():\n",
    "        ws.column_dimensions[col].width = width\n",
    "\n",
    "\n",
    "    ##--  Apply font color to all cells in column \n",
    "    font_color = \"0707C5\"  \n",
    "    for cell in ws['E']: \n",
    "        cell.font = Font(color=font_color)\n",
    "    # \n",
    "    font_color = \"c10105\"  \n",
    "    for cell in ws['J']: \n",
    "        cell.font = Font(color=font_color)\n",
    "\n",
    "\n",
    "    ##-- Loop through each cell in the column_\"E\" >> 'text'  and replace CR+LF    #IP20241125\n",
    "    #    also, set alignments\n",
    "    for row in ws.iter_rows(min_col=5, max_col=5, min_row=2, max_row=ws.max_row):\n",
    "        for cell in row:\n",
    "            if isinstance(cell.value, str):  # Check if the cell contains text\n",
    "                # Replace CR (carriage return) and LF (line feed) with a space\n",
    "                cell.value = cell.value.replace('\\r\\n', ' ').replace('\\r', ' ').replace('\\n\\n', '\\n')\n",
    "                cell.alignment = Alignment(wrap_text=False, vertical=\"top\", horizontal=\"left\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #IP20241120  re-order columns  \n",
    "    #   \n",
    "    # Specify the column to move  \n",
    "    col_to_move_indx = 13    # N-of-clmn==(index)+1\n",
    "    col_to_insert_indx = 4\n",
    "    # Get all columns\n",
    "    columns = list(ws.columns)\n",
    "    col_to_move = columns[col_to_move_indx]\n",
    "    col_to_insert = columns[col_to_insert_indx]  \n",
    "    # Get the data in the column to move\n",
    "    col_data = [cell.value for cell in col_to_move]\n",
    "    # Remove the column from its current position \n",
    "    ws.delete_cols(col_to_move_indx+1)\n",
    "    # Insert the column at the destination position\n",
    "    ws.insert_cols(col_to_insert_indx)  \n",
    "    for row_idx, value in enumerate(col_data, start=1):\n",
    "        ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "\n",
    "    col_to_move_indx = 14    # N-of-clmn==(index)+1\n",
    "    col_to_insert_indx = 5\n",
    "    # Get all columns\n",
    "    columns = list(ws.columns)\n",
    "    col_to_move = columns[col_to_move_indx]\n",
    "    col_to_insert = columns[col_to_insert_indx]  \n",
    "    # Get the data in the column to move\n",
    "    col_data = [cell.value for cell in col_to_move]\n",
    "    # Remove the column from its current position \n",
    "    ws.delete_cols(col_to_move_indx+1)\n",
    "    # Insert the column at the destination position\n",
    "    ws.insert_cols(col_to_insert_indx)   \n",
    "    for row_idx, value in enumerate(col_data, start=1):\n",
    "        ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "\n",
    "    ##-- re-Set the column width AFTER moving columns  IP20241124 (preserve in code, if further column-moving will be changed)\n",
    "    column_widths = {\n",
    "        'A': 12, 'B': 19, 'C': 15,'D': 19, 'E': 19, 'F': 8, 'G': 35, 'H': 5, 'I': 5, 'J': 17, \n",
    "        'K': 17, 'L': 15, 'M': 19, 'N': 19, 'O': 25 , 'P': 7 , 'Q': 6, 'R': 37    \n",
    "    }\n",
    "    ##-- Apply the column widths\n",
    "    for col, width in column_widths.items():\n",
    "        ws.column_dimensions[col].width = width\n",
    "    \n",
    "    # IP20241124 move \"deactivated\" column\n",
    "    col_to_move_indx = 16    # N-of-clmn==(index)+1\n",
    "    col_to_insert_indx = 6\n",
    "    # Get all columns\n",
    "    columns = list(ws.columns)\n",
    "    col_to_move = columns[col_to_move_indx]\n",
    "    col_to_insert = columns[col_to_insert_indx]  \n",
    "    # Get the data in the column to move\n",
    "    col_data = [cell.value for cell in col_to_move]\n",
    "    # Remove the column from its current position \n",
    "    ws.delete_cols(col_to_move_indx+1)\n",
    "    # Insert the column at the destination position\n",
    "    ws.insert_cols(col_to_insert_indx)   \n",
    "    for row_idx, value in enumerate(col_data, start=1):\n",
    "        ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "    \n",
    "    \n",
    "    # IP20241124 move \"is_bot\" column\n",
    "    col_to_move_indx = 16    # N-of-clmn==(index)+1\n",
    "    col_to_insert_indx = 7\n",
    "    # Get all columns\n",
    "    columns = list(ws.columns)\n",
    "    col_to_move = columns[col_to_move_indx]\n",
    "    col_to_insert = columns[col_to_insert_indx]  \n",
    "    # Get the data in the column to move\n",
    "    col_data = [cell.value for cell in col_to_move]\n",
    "    # Remove the column from its current position \n",
    "    ws.delete_cols(col_to_move_indx+1)\n",
    "    # Insert the column at the destination position\n",
    "    ws.insert_cols(col_to_insert_indx)   \n",
    "    for row_idx, value in enumerate(col_data, start=1):\n",
    "        ws.cell(row=row_idx, column=col_to_insert_indx, value=value)\n",
    "\n",
    "    ##-- re-Set the column width AFTER moving columns\n",
    "    column_widths = {\n",
    "        'A': 12, 'B': 19, 'C': 15,'D': 19, 'E': 19, 'F': 7, 'G': 7, 'H':8, 'I':35, 'J': 5,   \n",
    "        'K': 5, 'L': 17, 'M': 17, 'N': 15, 'O': 19, 'P': 19, 'Q': 25, 'R': 37      \n",
    "    }\n",
    "    ##-- Apply the column widths\n",
    "    for col, width in column_widths.items():\n",
    "        ws.column_dimensions[col].width = width\n",
    "\n",
    "    ##-- Data align-to-left  IP2024124  (excluding 1st row)\n",
    "    for row in ws.iter_rows(min_col=10, max_col=11, min_row=2, max_row=ws.max_row):\n",
    "        for cell in row:\n",
    "            cell.alignment = Alignment(horizontal='center')   # 'left'\n",
    "            if isinstance(cell.value, (int, float)):\n",
    "                cell.font = Font(size=12, bold=True)\n",
    "    \n",
    "    #\n",
    "    #  first row (Row 1) formattings\n",
    "    ##-- Freeze the first row (Row 1)\n",
    "    ws.freeze_panes = 'A2'\n",
    "    ##-- Set font size and bold for the first row\n",
    "    font = Font(size=9, bold=True)\n",
    "    ##-- Set the height of the first row\n",
    "    ws.row_dimensions[1].height = 43 \n",
    "    ##-- Define the RGB color\n",
    "    fill = PatternFill(start_color=\"e7c9fb\", end_color=\"e7c9fb\", fill_type=\"solid\")\n",
    "    ##-- Apply the color, font formatting to the 1st row (Header row)\n",
    "    for cell in ws[1]:\n",
    "        cell.font = font\n",
    "        cell.fill = fill\n",
    "        #cell.alignment = Alignment(wrap_text=True) # Set wrap text for the cells in the first row \n",
    "        cell.alignment = Alignment(wrap_text=True, vertical=\"top\", horizontal=\"left\")\n",
    " \n",
    "    font_color = \"c10105\"  #IP font_color User_name\n",
    "    for cell in ws['E']: \n",
    "        cell.font = Font(color=font_color)\n",
    "\n",
    "    #IP20241121  fill_color when  -> \"is_bot\"==True  -> message's \"type\"==\"thread\"\n",
    "    fill_bot = PatternFill(start_color=\"FBBF8F\", end_color=\"FBBF8F\", fill_type=\"solid\")\n",
    "    fill_thread = PatternFill(start_color=\"FBFB99\", end_color=\"FBFB99\", fill_type=\"solid\")\n",
    "    last_row = ws.max_row\n",
    "    for i in range(2, last_row + 1):\n",
    "        if ws[f'g{i}'].value == \"True\" or ws[f'g{i}'].value == True:\n",
    "            for col in ['C', 'D', 'E', 'F', 'G']:\n",
    "                ws[f'{col}{i}'].fill = fill_bot\n",
    "        if ws[f'H{i}'].value == \"thread\":\n",
    "            for col in ['H', 'I']:\n",
    "                ws[f'{col}{i}'].fill = fill_thread\n",
    "\n",
    "\n",
    "    ##-- Delete columns  ::   json_name \tjson_mod_date\tchannel_folder    #IP20241125\n",
    "    #    this columns is tech only, for development and debug, not for PMs\n",
    "    ws.delete_cols(15)  # Delete column 'O' (now 'P' has shifted to 'O')\n",
    "    ws.delete_cols(15)  # Delete column former 'P'  \n",
    "    ws.delete_cols(15)  # Delete column former 'Q' \n",
    "\n",
    "\n",
    "    #\n",
    "    ##-- Rename the sheet\n",
    "    ws_title = curr_channel_name \n",
    "    ws_title = ws_title[:31]\n",
    "    ws.title = ws_title \n",
    "    #\n",
    "    ##-- Save the changes to the Excel file\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ae148b4-3f0b-4f44-9d6b-abe0df76ca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:27:27.571419 Started analysis after sanity checks\n",
      "slackexport_folder_path =>> /home/agds/Documents/RebeccaEverleneTrust/RebeccaEverlene_Slack_export\n",
      "14:27:27.613456 Obtained channels_df\n",
      "14:27:27.885590 Obtained users_df\n",
      "14:27:27.910859 Wrote channels_df to xlsx file\n",
      "14:27:28.053427 Wrote users_df to xlsx file\n",
      "14:27:28.053447 Starting loop over channels \n",
      "\n",
      "general 14:27:28.053457  Set-up channel name and path to directory\n",
      "general 14:27:30.732698  Collected channel messages from the json files\n",
      "general 14:27:30.760639  Collected users in current channel\n",
      "general 14:27:32.105875  Included the users information on channel_messages_df\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(curr_channel_name, datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Included the users information on channel_messages_df\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m##-- Replace user and team identifiers with their display_names whenever present in a message:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#user_id_to_name(channel_messages_df, users_df) \u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m user_id_to_name(channel_messages_df, users_df) \n\u001b[1;32m     58\u001b[0m channel_id_to_name(channel_messages_df, users_df)\n\u001b[1;32m     59\u001b[0m parent_user_id_to_name(channel_messages_df, users_df) \u001b[38;5;66;03m#AG20241122: routine defined in its own function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 293\u001b[0m, in \u001b[0;36muser_id_to_name\u001b[0;34m(df_messages, df_users)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# AG20241122: begin\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m df_users[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;129;01mor\u001b[39;00m user \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSLACKBOT\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB043CSZ0FL7\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 293\u001b[0m     name \u001b[38;5;241m=\u001b[39m df_users[df_users[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39muser][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    294\u001b[0m     is_bot \u001b[38;5;241m=\u001b[39m df_users[df_users[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39muser][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_bot\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]   \n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_bot\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "##-- Main analysis:\n",
    "if continue_analysis==False:\n",
    "    print(\"Please review the input information\")\n",
    "else:    \n",
    "    print(datetime.now().time(), 'Started analysis after sanity checks' )\n",
    "    print(\"slackexport_folder_path =>> \"+slackexport_folder_path)\n",
    "    ##-- Extract the channels and users information into dataframes:\n",
    "    channels_df = get_all_channels_info(slackexport_folder_path)\n",
    "    print(datetime.now().time(), 'Obtained channels_df')\n",
    "    users_df = get_all_users_info(slackexport_folder_path)\n",
    "    print(datetime.now().time(), 'Obtained users_df')\n",
    "    \n",
    "    ##-- Write all channel's info to .xlsx files, if requested by user:\n",
    "    if write_all_channels_info==True:\n",
    "        slack_export_channel_filename = \"_all_channels\"\n",
    "        slack_export_channel_folder_path_xlsx = f\"{converted_directory}/{slack_export_channel_filename}{'.xlsx'}\"\n",
    "        channels_df.to_excel(slack_export_channel_folder_path_xlsx, index=False)\n",
    "        print(datetime.now().time(), 'Wrote channels_df to xlsx file')  \n",
    "    \n",
    "    ##-- Write all users's info to .xlsx files, if requested by user:\n",
    "    if write_all_users_info==True:\n",
    "        slack_export_user_filename = \"_all_users\"        \n",
    "        slack_export_user_folder_path_xlsx = f\"{converted_directory}/{slack_export_user_filename}{'.xlsx'}\" #_IP\n",
    "        users_df.to_excel(slack_export_user_folder_path_xlsx, index=False) #_IP\n",
    "        print(datetime.now().time(), 'Wrote users_df to xlsx file')\n",
    "\n",
    "    ##-- Iterate over channel's folders:\n",
    "    dfs_list = []\n",
    "    print(datetime.now().time(), 'Starting loop over channels', '\\n')\n",
    "    for i_channel in range(len(channels_names)):\n",
    "\n",
    "        ##-- Define the name of the current channel and the source path containing its json files:\n",
    "        curr_channel_name = channels_names[i_channel] \n",
    "        parentfolder_path = f\"{slackexport_folder_path}/{curr_channel_name}\" \n",
    "        print(curr_channel_name, datetime.now().time(), ' Set-up channel name and path to directory')\n",
    "        \n",
    "        ##-- Collect all the current_channel's messages in channel_messages_df through the function get_channel_messages_df:\n",
    "        json_list = all_channels_jsonFiles_dates[i_channel]\n",
    "        channel_messages_df = get_channel_messages_df(slackexport_folder_path, curr_channel_name, json_list)  \n",
    "        print(curr_channel_name, datetime.now().time(), ' Collected channel messages from the json files')\n",
    "        #\n",
    "        #IP20241121 move to separate folders-without-messages-JSONs\n",
    "        if len(channel_messages_df)<1:\n",
    "            print(\"for the folder \",curr_channel_name,\"messages_number= \",len(channel_messages_df),\"there is no channel's folder\", '\\n')\n",
    "            continue    \n",
    "\n",
    "        ##-- Collect all the users in the current channel through the function get_channel_users_df:\n",
    "        channel_users_df = get_channel_users_df(channel_messages_df, users_df )\n",
    "        print(curr_channel_name, datetime.now().time(), ' Collected users in current channel')\n",
    "        \n",
    "        ##-- Use channel_users_df to fill-in the user's information in channel_messages_df: \n",
    "        add_users_info_to_messages(channel_messages_df, channel_users_df)\n",
    "        print(curr_channel_name, datetime.now().time(), ' Included the users information on channel_messages_df')\n",
    "        \n",
    "        ##-- Replace user and team identifiers with their display_names whenever present in a message:\n",
    "        #user_id_to_name(channel_messages_df, users_df) \n",
    "        user_id_to_name(channel_messages_df, users_df) \n",
    "        channel_id_to_name(channel_messages_df, users_df)\n",
    "        parent_user_id_to_name(channel_messages_df, users_df) #AG20241122: routine defined in its own function\n",
    "        print(curr_channel_name, datetime.now().time(), \" User's id replaced by their names in messages\")\n",
    "\n",
    "        ##-- Extract hyperlinks from messages, if present (extracted as a list; edit if needed):\n",
    "        extract_urls(channel_messages_df)\n",
    "        print(curr_channel_name, datetime.now().time(), ' URLs extracted from messages')\n",
    "\n",
    "        ##-- Change format of the time in seconds to a date in the CST time-zone: (Pending 'ts_latest_reply' and 'ts_thread'!)\n",
    "        #channel_messages_mindate = pd.to_datetime(np.float64(channel_messages_df['ts']), unit='s').min().date()   #AG20241120: Can be deleted\n",
    "        #channel_messages_maxdate = pd.to_datetime(np.float64(channel_messages_df['ts']), unit='s').max().date()   #AG20241120: Can be deleted\n",
    "        ts_to_tz(channel_messages_df, 'ts', 'msg_date')\n",
    "        ts_to_tz(channel_messages_df, 'json_mod_ts', 'json_mod_date')\n",
    "        ts_to_tz(channel_messages_df, 'ts_latest_reply', 'latest_reply_date')\n",
    "        ts_to_tz(channel_messages_df, 'ts_thread', 'thread_date')\n",
    "        print('main_analysys ->>',curr_channel_name, \"  \", datetime.now().time(), ' Formated the dates and times in the dataframe')\n",
    "            \n",
    "        ##-- Reorder the columns in channel_messages_df, if necessary:\n",
    "        #channel_messages_df = channel_messages_df[['channel', 'json_name', 'json_mod_date', 'user', 'name', 'display_name', 'ts', 'msg_id', 'type', 'text']]\n",
    "        #channel_messages_df.index = ['']*len(channel_messages_df)\n",
    "        \n",
    "        ##-- Sort the dataframe by msg_date:\n",
    "        channel_messages_df.sort_values(by='msg_date', inplace=True, ignore_index=True)\n",
    "        \n",
    "        ##-- Write channel_messages_df to a .xlsx file:\n",
    "        channel_messages_mindate = channel_messages_df['msg_date'].min().split(\" \")[0]\n",
    "        channel_messages_maxdate = channel_messages_df['msg_date'].max().split(\" \")[0]\n",
    "        #channel_messages_maxdate = channel_messages_df['msg_date'].max().split(\" \")[0]\n",
    "        channel_messages_filename = f\"{curr_channel_name}_{channel_messages_mindate}_to_{channel_messages_maxdate}\"\n",
    "        channel_messages_folder_path = f\"{converted_directory}/{channel_messages_filename}.xlsx\"\n",
    "        channel_messages_df.to_excel(f\"{channel_messages_folder_path}\", index=False)\n",
    "        apply_excel_adjustments(f\"{channel_messages_folder_path}\",curr_channel_name)  #AG: defined this routine in the function apply_excel_adjustments\n",
    "        print(curr_channel_name, datetime.now().time(), ' Wrote curated messages to xlsx files', '\\n')\n",
    "\n",
    "        dfs_list.append(channel_messages_df)\n",
    "        \n",
    "print(datetime.now().time(), 'Done')\n",
    "\n",
    "channel_messages_df.head()   # for debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6d865-476d-4774-be0c-62fa71b6b427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
